---
title: "From Betting Lines to Bottom Lines: Improving MMA Fight Outcome Forecasts with Advanced Analytics"
subtitle: "Kyle VanHatten, Bachelors of Arts Candidate in Statistics and Data Science at Yale University"
author:
  - "Advised by Joseph Chang"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format: 
  pdf:
    documentclass: article
    toc: false
    number-sections: true
  html:
    toc: true
    toc-depth: 2
    number-sections: true
header-includes:
  - \usepackage{array}
fontsize: 11pt
---

\newpage

# Abstract

  This paper examines whether incorporating novel variables and analytical methods can improve the prediction of mixed martial arts (MMA) fight outcomes beyond what is implied by betting odds. We begin by cleaning and merging multiple data sources, ensuring a high-quality dataset for analysis. Using feature engineering, we create variables to capture fighter styles, activity levels, finishing abilities, and latent factors through factor analysis. These new variables are incorporated into logistic regression, discriminant analysis, and machine learning models, including random forests and XGBoost. Compared to a baseline model using only the betting odds, our "super" model, which includes these novel predictors, demonstrates statistically significant improvements in predictive accuracy. We conduct cross-validation and likelihood ratio tests, confirming that the added complexity yields robust and meaningful gains. Random forest and XGBoost feature importance measures highlight the value of stylistic and performance-related attributes. While the improvements are modest, they suggest that the betting market does not fully integrate all available fighter information. Additionally, we explore the impact of a 2017 rule change in judging criteria on model accuracy, finding no statistically significant shift in performance differences. Overall, this study indicates that combining betting odds with nuanced fighting metrics can enhance predictive modeling in MMA.

\newpage

# Introduction

  Mixed Martial Arts (MMA) has evolved from its origins as a competition between various martial arts disciplines into a dynamic and complex sport, requiring a wide range of skills and strategies. MMA has continued to grow in popularity, especially in sports betting, the market has witnessed rapid expansion, particularly with platforms like DraftKings, the official sports betting platform of the UFC. Betting markets have long been regarded as generally being "efficient" (Winkelmann), which can be understood as the markets fully including and capturing all relevant information regarding predicting said chance events. For the most part, this is true, but betting on mixed martial arts Fighting (MMA) presents an interesting case in which this efficiency assumption might be tested. The sport's complexity, with its wide array of fight styles and the unpredictability of each bout, offers an opportunity to explore potential inefficiencies in betting odds and predictions. It is well understood that all fighters competing in MMA, even those facing an opponent of greater skill, have a 'punchers chance,' or a chance to score a surprise knockout or submission victory. Specific abilities or actions can often make up for large skill gaps. Much like in the Super Bowl, 'worse' MMA fighters regularly can win large upsets, sometimes dominantly. This paper seeks to investigate whether MMA fight outcomes can be better predicted by leveraging fighter statistics, advanced statistical techniques, and machine learning models. 

  MMA, such as many other sports, has become increasingly data and metric-based when discussing analysis, and hundreds of statistics for each fighter can be easily attained before any given fight. Given that these data are publically available, it stands to reason that new novel variables and techniques may be needed to achieve more accurate predictions. As such, there is great potential to improve prediction accuracy over standard betting lines. The sport of Mixed Martial Arts (MMA) has evolved from its origins as a competition between various martial arts disciplines into a dynamic and complex sport, requiring a wide range of skills and strategies. MMA has continued to grow in popularity, especially in the realm of sports betting, the market has witnessed rapid expansion, particularly with platforms like DraftKings, the official sports betting platform of the UFC. Betting markets have long been regarded as generally being "efficient" (Winkelmann), which can be understood as the markets fully including and capturing all relevant information with regards to predicting said chance events. For the most part, this is true, but betting on Mixed-Martial-Arts Fighting (MMA) presents an interesting case in which this efficiency assumption might be tested. The sport's complexity, with its wide array of fight styles and the unpredictability of each bout, offers an opportunity to explore potential inefficiencies in betting odds and predictions. It is well understood that all fighters competing in MMA, even those facing an opponent of greater skill, have a 'punchers chance,' or a chance to score a surprise knockout or submission victory. Specific abilities or actions can often make up for large skill gaps. Much like in the Super Bowl, 'worse' MMA fighters regularly can win large upsets, sometimes dominantly. This paper seeks to investigate whether MMA fight outcomes can be better predicted by leveraging fighter statistics, advanced statistical techniques, and machine learning models. MMA, such as many other sports, has become increasingly data and metric-based when discussing analysis, and hundreds of statistics for each fighter can be easily attained before any given fight. Given that these data are publically available, it stands to reason that new novel variables and techniques may be needed to achieve more accurate predictions. As such, there is great potential to improve prediction accuracy over standard betting lines.


  The objective of this study is to analyze pre-fight fighter statistics across a wide range of MMA fights, construct innovative metrics such as activity scores and style clusters, and evaluate the predictive power of various models. Specifically, this research will attempt to better understand which features are most predictive of MMA fight outcomes, which models strike the best balance between accuracy and complexity, for predicting fight results, and how dynamic factors like age, fighting style, and rule changes impact the process of predicting fight outcomes. In doing so, this thesis will contribute to the growing field of sports analytics (Chugani) by integrating and cleaning a comprehensive data set, developing several novel features, and thoroughly evaluating statistical and machine learning models to provide deeper insights into the factors that drive fight outcomes in MMA. 

  This paper has been made as a QMD /Quarto document using R and R Studio, with all code and some results being omitted. A full qmd file containing all code and results can be found at the following GitHub repository [**here**](https://github.com/KyleVanHatten/KVanHatten_SeniorThesis.git)  The data set was gathered from Kaggle and can be found [**here**](https://www.kaggle.com/datasets/rajeevw/ufcdata)(Warrier)and [**here**](https://www.kaggle.com/datasets/mdabbert/ultimate-ufc-dataset)(mdabbert) as of 12/11/2024.

  MMA began as a competition aimed at determining the most effective martial art by bringing together practitioners from various disciplines under a common rule set. In MMA fighters combine techniques and skills from disciplines such as Brazilian Jiu-Jitsu, boxing, kickboxing, wrestling, and many others, often adapting to the unique challenges posed by their opponent's style. As time has gone on, MMA fighters have needed to become increasingly well-rounded in all skills to achieve success at the highest level. Fights are won either by finish or going to a judge's decision. Finishes can happen at any moment within a fight (typically 3 5-minute rounds), whereas winners are decided by the ringside judges if a fight does not reach a finish before time runs out. A finish could involve one fighter rending the other unconscious (Knockout, or KO), causing one fighter to no longer be able to intelligently defend themself (Technical Knockout), or "tapping out" are giving up to a submission, such as a choke hold or an arm lock (Submission, or SUB). The fact that fights can end at any moment, from the 5th second of the fight to the very last second (Record Book) makes the sport exhilarating and unpredictable. Additionally, since competition only occurs between two individuals rather than teams, and these competitions occur only a few times a year for each fighter (typically only three), this adds to the unpredictability of predicting fight outcomes. In *A Markov Chain Model for Forecasting Results of Mixed Martial Arts Contests* (2022), the authors note the impact of infrequent fights and inconsistent times between fights as factors contributing to noise within the data(Holmes et al.). Finally, the sport is further complicated by the wide variety of martial arts being used as this makes judging rounds and deciding the winner of a fight more complicated than in a sport like boxing, which has a smaller variety of actions occurring and a more consistent scoring pattern. The wide variety of styles also impacts how fighters with different styles match up, similar to a game of rock-paper-scissors. By better understanding which pre-fight statistics contribute to understanding which fighter is going to win, this paper lays the groundwork to improve the quality/focus of preparing for fights, making more informed betting decisions, and gaining insight into the nature of betting markets on outcomes with extreme variation and noise.


\newpage

# Literature Review

  Much has been written about sports betting and betting markets, but relatively little has been written about MMA sports betting, or predicting MMA fight outcomes in general. One major challenge to predicting MMA betting lies in the fact that many of the "variables" or factors that could influence the outcome of a fight are unknown to the broader public such as injuries occurring in the lead-up to a fight. An example of this occurring happened in 2022 in which the Fighter TJ Dillashaw repeatedly injured his shoulder in the lead-up to a fight with the divisional champion, who would win the fight dominantly after Dillashaw would once again re-injure his shoulder at the beginning of the match. A lot of data is also simply unknowable, such as what specific choices a fighter will make during a fight. One high-profile example of this is Chris Weidman choosing to throw a spinning backkick in his match against Luke Rockhold. This technique was risky, and well out of the ordinary for Weidman, and would result in a change in a position and a dominant loss in a fight he was previously winning. Finally, many intangibles permeate the sport and cannot or have not been quantified technically. For example, as mentioned in the introduction, the "style" of fighters is not something that has not been objectively measured using pre-fight data. The goal of this literature review, and subsequent analysis, is to target this third factor by trying to make intangible qualities such as "style", more tangible.

  In one recent work, *A Markov chain model for forecasting results of mixed martial arts contests* (Holmes et al), the author analyzes MMA fights by simulating continuous Markov chains rather than predicting the binary outcome of the fight directly. The resulting model leads to an increase in accuracy over the traditional betting odds implied win probabilities. The paper also makes note of some of the major papers covering MMA forecasting and the implementation of machine learning models saying, "There is very little literature on forecasting the results of MMA bouts. Johnson (2009), Ho (2013), Hitkul et al. (2019), and Robles and Wu (2019) used various machine learning algorithms consisting of similar variables to predict the winner of a fight" (Holmes et al). These models ranged in the effectiveness of predictions of 50-68% but notably weren't compared to the betting market predictions. *Are Money Line Odds in UFC Matches Calibrated? Evidence From Events in 2019-2020* (Stanhope) notably looks at recent UFC fight outcomes to determine if the odds placed on matches are calibrated. The paper concludes that money line wagers (betting on who will win) are indeed calibrated, but wagers focusing on winning methods or rounds were not (Stanhope), suggesting a layer of inefficiency. One possible explanation for this could be smaller betting volumes on those more specific bets. This paper will explore the use of clustering (k-means and hierarchical) to categorize fighters into different "styles" and assess how these styles match up in head-to-head matchups, a concept also examined by Robles and Wu (2019). Their work provides valuable insight into this approach, which will be further explored and expanded upon in the current analysis.

  Broadly, studies of betting market efficiency, such as 'Market Efficiency and the Favorite-Longshot Bias' (1994), explore biases that might affect MMA betting markets as well. The paper in question analyzes the favorite-longshot bias observed in horse-racing, in which bettors over-bet/over-value the chances of extremely unlikely outcomes, and finds this pattern to also exist in baseball*(Woodland and Woodland)*. *Determinants of betting market efficiency* (2006) meanwhile sought to better understand betting market efficiency with regards to horse racing by incorporating regression analysis, ultimately finding that the favorite-longshot bias exists but is diminished by larger pools of racers, lower quality fields, and races occurring on grass (Gramm \* and Owens). *Weak Form Efficiency in Sports Betting Markets* (2023) analyzes the efficiency of sports betting markets by looking at betting odds and outcomes across a wide array of sports over an extended period, finding that sports-betting markets are generally efficient outside of specific situation bets which remained net-positive over the analysis period (Robbins). Testing Market Efficiency: Evidence From The NFL Sports Betting Market (2012) similarly looks at NFL match outcomes using a probit model, finding that although some specific betting strategies were capable of generating statistically significant profits, any inefficiency tended to dispensary and weaken over time. This paper aims to build on existing literature by developing novel variables, testing both statistical and machine learning models, and exploring their effectiveness in predicting MMA fight outcomes. By explicitly comparing predictions from created models to the predictions implied by the betting odds, we will directly attempt to find market inefficiency and pinpoint exactly which existing or unaccounted-for / created variables are responsible for driving the inefficiency.

\newpage

# Data Processing and Variable Development

## Datasets

  The two key data sets were each gathered from Kaggle, and can be accessed [**here**](https://www.kaggle.com/datasets/rajeevw/ufcdata)(Warrier)and [**here**](https://www.kaggle.com/datasets/mdabbert/ultimate-ufc-dataset)(mdabbert) as of 12/11/2024. In both of the data sets, each row represents a unique fight. The former data set spans from 1993 (the year the UFC was founded) to 2021 and contains information on pre-fight statistics for each fighter, attributes of the fighters such as their height or wingspan (also known as reach), and winner and method of victory for the fight in question. The latter data set spans from 2010 to June 2024, and contains similar pre-fight statistics and winner information, but also includes the respective pre-fight betting odds. These odds were gathered from bestfightodds.com, which compiles odds from the largest online betting markets. For this datasheet specifically, only the DraftKings odds are included and no averaging or other weighting occurs. Note that these data include information from only the fighters UFC fights in the period and is does heir fight in other organizations, or their amateur career.

  These data sets were merged horizontally by matching the respective fighters, as well as the date to ensure that pairs who fought multiple times are included. As a small note, it is important to note that for all UFC fights, one fighter is assigned as the "red fighter" (or they are in the "red corner"), and the other as the blue fighter. It is a convention for the higher-ranked fighter or the champion of a weight division to be the red-fighter. When there is not a clear distinction in the ranking (i.e. neither fighter is ranked officially in the division), the fighter with more experience is typically red. Typically, red fighters tend to be the favorite in a given fight as a result. The main variables to consider for this data set are "winner", which is a binary variable that is made to be true for "Red" fighter wins, and false for "Blue" fighter wins, and "r_odds", which describes the implied probability of the "Red" fighter winning the fight. In this analysis, we standardize the r_odds so that r_odds and b_odds sum to 1.

## Data Cleaning

  Our analysis began by loading in the required libraries. Libraries such as caret and randomForest were selected to enable machine learning workflows, while MASS and psych support statistical modeling and factor analysis. Below is a section doing so, with comments indicating the general purpose of each library. It is important to install each package before running the analysis. As reproducibility is a cornerstone of good statistical practice, we also use the set.seed function to ensure that random processes such as clustering or random forest algorithms produce consistent results, allowing others to replicate our findings exactly. Finally, several utility functions are included in order to simplify later code. The select_vars_with_substring function was designed in order to obtain a list of variables including the specified search term, as well as the "winner" variable. This allowed for easy subletting of data which was helpful for regressing specific groups of variables on the winner variable. The vegas_to_decimal_chance function provides a way of turning the Vegas-style odds into decimal probability values. The get_top_variables function allows for easy access to the key variables impacting the factors in the factor analysis section of the paper. Next, the calc_vif function calculates the variance inflation factor for different variables, which was useful when wanting to build models with limited multicollinearity. Finally, log_likelihood was used to compute log-likelihoods from probabilities.

```{r,include=FALSE}
# This code section loads libraries and creates a select_vars_with_substring which can be used to quickly return a vector of variable names for running simple regressions on subsets of data. 
# -----------------------------------------------------------------------------------

rm(list=ls())
# Hides Warnings
options(warn = -1)

# Load required libraries
library(tidyverse) # Data wrangling, visualization
library(janitor) # Data cleaning
library(dplyr) # Data manipulation
library(tidyr) # Data tidying
library(MASS) # Statistical models
library(mice) # Missing data imputation
library(randomForest) # Random forest model
library(xgboost) # Gradient boosting
library(caret) # Machine learning framework
library(glmnet) # Regularized GLMs
library(ggplot2) # Data visualization
library(stargazer) # Regression tables
library(broom) # Tidy model output
library(stats) # Basic statistics
library(car) # Regression diagnostics
library(psych) # Psychometrics, factor analysis
library(car) # For regression diagnostics, ANOVA, and data manipulation utilities
library(psych) # For psychometric analysis, including factor analysis and descriptive statistics
library(MVN) # For multivariate normality testing
library(biotools) # For multivariate normality testing
library(QuantPsyc) # For multivariate normality testing
library(kableExtra) # Tables 

set.seed(123)  

# Function: Select columns containing a specific substring and ensure "winner" is included
select_vars_with_substring <- function(df, substring) {
  selected_columns <- grep(substring, names(df), value = TRUE)  # Get column names containing substring
  selected_columns <- unique(c("winner", selected_columns))  # Ensure "winner" is included
  return(selected_columns)  # Return selected column names
}

# Convert betting odds to standardized probabilities
vegas_to_decimal_chance <- function(odds) {
  ifelse(odds > 0, 100 / (odds + 100), -odds / (-odds + 100))
}

# Define a function to extract the top variables for each factor
get_top_variables <- function(loadings_matrix, num_top = 5) {
  top_variables <- apply(loadings_matrix, 2, function(column) {
    sorted_indices <- order(abs(column), decreasing = TRUE)
    names(column)[sorted_indices[1:num_top]]
  })
  return(top_variables)
}

# Function to calculate Variance Inflation Factor (VIF)
calc_vif <- function(data) {
  vif_values <- sapply(names(data), function(var) {
    model <- lm(as.formula(paste(var, "~ .")), data = data)
    1 / (1 - summary(model)$r.squared)
  })
  return(vif_values)
}

# Define a function to calculate log-likelihood
log_likelihood <- function(y, p) {
  return(sum(y * log(p) + (1 - y) * log(1 - p), na.rm = TRUE))
}

```

  The next step of analysis involved loading the data into R, standardizing ID variables between the two data sets, and merging the data sets horizontally. From then, the resulting date frame was cleaned and standardized. Column names were standardized using patterns like replacing 'Red' with 'R\_' and 'Blue' with 'B\_' to ensure consistency and clarity. Draws, although not nonexistent, are relatively rare in MMA. As MMA fights occur with an odd number of rounds, special circumstances must occur for a draw to occur (either a very dominant round from one fighter or a point deduction from commuting a foul or several fouls). As they make up a small proportion of fight outcomes(approximately 5%), and complicate what would otherwise be a simple binary winner variable, they have been removed for this specific analysis. This decision simplifies the predictive modeling process as binary variables are much easier to work with, but presents an opportunity for future research to include draws as a possible outcome. Columns that were deemed redundant, error-prone, or irrelevant for the analysis were removed to streamline the data set.

  Odds for the probability of red and blue victories are also converted from "Vegas odds" to decimal values of the implied probability from said odds. From then, the data set is filtered to ensure completeness for variables which will later be used to classify fighters based on their styles. Weight classes were reordered to be sorted by gender and then weight class in ascending order of weight. More information on weight classes in MMA is available [here](https://www.mmahive.com/ufc-weight-classes/) (Bloom). Variables other than winners which describe the row's fight outcomes specifically, rather than pre-fight statistics, are removed as these are not useful for for prediction. Winning methods (how a given fight was won e.g., KO or submission) were also removed to prevent circular reasoning in the predictive models, as they directly relate to fight outcomes. With all of these done, we have concluded the data-cleaning process. This data cleaning and pre-processing process ensures that the resulting dataset is both high-quality and contributes to the specific objectives of this study.

```{r, include=FALSE}
# This code section loads the original data from each source, standardizes them, merges them, and then cleans the data. 
# -----------------------------------------------------------------------------------

# Load and preprocess data
processed_data <- read.csv("C:/Users/kylou/Downloads/archive (2)/data.csv")
master_data <- read.csv("C:/Users/kylou/Downloads/UFCDATA/ufc-master.csv")

# Standardize column names for consistency
colnames(master_data) <- colnames(master_data) %>%
  gsub("Red", "R_", .) %>%
  gsub("Blue", "B_", .) %>%
  gsub("Fighter", "fighter", .) %>%
  gsub("Date", "date", .)

# Filter out draws and merge datasets by common keys
processed_data <- processed_data[processed_data$Winner != "Draw", ]
merged_data <- merge(master_data, processed_data, by = c("R_fighter", "B_fighter", "date"))

# Clean merged dataset
merged_data <- merged_data %>%
  mutate(winner = (Winner.x == "Red"))  # Create binary "winner" column (1 = Red win)

# Remove specific columns
merged_data <- merged_data %>%
  dplyr::select(
    -R_DecOdds, -B_DecOdds, -RSubOdds, -BSubOdds, -RKOOdds, -BKOOdds, 
    -R_Weight_lbs, -B_Weight_lbs, -R_ExpectedValue, -B_ExpectedValue
  )

# Clean column names
cleaned_data <- merged_data %>%
  clean_names() %>%  # Ensure all column names are consistent
  dplyr::select(-contains("_2"), -contains("_x"), -contains("_y"))  # Remove unwanted columns

# Convert betting odds to standardized probabilities

cleaned_data <- cleaned_data %>%
  mutate(
    r_odds = vegas_to_decimal_chance(r_odds),
    b_odds = vegas_to_decimal_chance(b_odds),
    total_chance = r_odds + b_odds,
    r_odds = r_odds / total_chance,
    b_odds = b_odds / total_chance
  )

# Removal total chance variable 
cleaned_data <- cleaned_data %>%
   dplyr::select(-total_chance)



# Filter for complete cases on key variables
key_vars <- c(
  "b_avg_sig_str_att", "b_avg_sig_str_landed", "b_avg_td_att", "b_avg_td_landed", 
  "b_avg_sub_att", "b_avg_ctrl_time_seconds", "b_wins_by_ko", "b_wins_by_submission",
  "r_avg_sig_str_att", "r_avg_sig_str_landed", "r_avg_td_att", "r_avg_td_landed", 
  "r_avg_sub_att", "r_avg_ctrl_time_seconds", "r_wins_by_ko", "r_wins_by_submission"
)

cleaned_data <- cleaned_data %>% filter(complete.cases(dplyr::select(., all_of(key_vars))))

# Reorder levels of the 'weight_class' factor
cleaned_data$weight_class <- factor(
  cleaned_data$weight_class,
  levels = c(
    "Women's Strawweight", "Women's Flyweight", "Women's Bantamweight", "Women's Featherweight",
    "Flyweight", "Bantamweight", "Featherweight", "Lightweight", "Welterweight", 
    "Middleweight", "Light Heavyweight", "Heavyweight"
  )
)

# Ensure 'date' is in Date format
cleaned_data <- cleaned_data %>%
  mutate(date = as.Date(date))


# Define the variables describing the current fight
potential_current_fight_vars <- c(
  "finish", "finish_details", "finish_round", "finish_round_time", 
  "total_fight_time_secs", "referee"
)

# Remove these variables from the cleaned_data dataset
cleaned_data <- cleaned_data %>%
  dplyr::select(-all_of(potential_current_fight_vars))


# Identify variables describing the current fight
vars_to_remove <- grep("^b_win_by_|^r_win_by_", names(cleaned_data), value = TRUE)

# Subset the data, excluding the identified variables
cleaned_data <- cleaned_data %>%
  dplyr::select(-all_of(vars_to_remove))



```

  In the following section, new simple variables are added to the data frame. First, the code adds a binary variable indicating whether or not each fighter is 35 years old or older. In "MMA", there is a known phenomenon of fighters turning 35 and then rapidly dropping in success. In weight classes at or below the 170lb division (Welterweight), fighters are 3-33 in title fights when they are 35 or older and their opponent is not (Barrasso)(Barrasso's figure was 2-31, and this has been updated to reflect the changes that occured in 2024). The inclusion of a binary age serves to extend the literature on age-related performance declines in MMA (Kirk), providing a simple yet effective feature for understanding its impact on fight outcomes which can be included in addition to pure age. The code also creates a subset where both fighters have had at least two prior fights. As contracts in the UFC are not fully guaranteed, often time fighters sign with the UFC, lose one or two fights, and are cut from the organization. It is possible that these particularly weak fighters could distort models, so this subset that excludes them was developed to test that. While excluding fighters with fewer than two fights could potentially reduce noise, it may also introduce bias by omitting potentially skilled newcomers who succeed in their first UFC fights and lowering the power by excluding the first two fights of all fighters that remain in the post-two fight subset. Although the remainder of the analysis will continue with the full dataset, the subset can potentially be a valuable robustness check to ensure that models are not disproportionately influenced by inexperienced fighters. While not included in this report, it was generally observed that this had little or negative impact on predicting power, aside from one exception which will be noted in the finishing score section.

```{r, include=FALSE}
# This code section creates a new subset filters for fights where both fighters have had at least 2 prior fights.
# -----------------------------------------------------------------------------------

# Step 1: Add age dummy variables
cleaned_data <- cleaned_data %>%
  mutate(
    r_age_over_34 = ifelse(r_age >= 35, 1, 0),
    b_age_over_34 = ifelse(b_age >= 35, 1, 0)
  )


# Step 2: Filter fights where both Red and Blue fighters have had at least 2 previous fights
high_fight_subset <- cleaned_data %>%
  filter(
    # Apply a condition row-wise to calculate the number of prior fights for each fighter
    sapply(1:nrow(cleaned_data), function(i) {
      # Count prior fights for Red fighter
      r_fighter_count <- sum(
        (cleaned_data$r_fighter == cleaned_data$r_fighter[i] | cleaned_data$r_fighter == cleaned_data$b_fighter[i]) & 
        cleaned_data$date < cleaned_data$date[i]
      )
      # Count prior fights for Blue fighter
      b_fighter_count <- sum(
        (cleaned_data$b_fighter == cleaned_data$r_fighter[i] | cleaned_data$b_fighter == cleaned_data$b_fighter[i]) & 
        cleaned_data$date < cleaned_data$date[i]
      )
      # Ensure both fighters have at least 2 prior fights
      return(r_fighter_count >= 2 & b_fighter_count >= 2)
    })
  )

```

  Next, it is important to note issues with the weights_lbs variables, which describe the respective fighters' weights at the official pre-fight weigh-in the day before the fight. Somehow, the authors of the original data set did not scrape the data correctly, as each fighter's most recent weight in the data overwrites all previous weights (ie, there is no individual variation in weight across different fights). This results in a lack of individual variation in weight across fights, rendering these variables unreliable. For this analysis, we will simply disregard this, but future work could involve re-scraping the data correctly.

```{r, include=FALSE}
# This code section examines inconsistencies in the r_weight_lbs and b_weight_lbs variables. 
# It creates tables to show the number of unique weight values per fight and demonstrates that these variables 
# are unreliable. 
# This code section also removes the unreliable r_weight_lbs and b_weight_lbs variables from cleaned_data.
# -----------------------------------------------------------------------------------

# Step 1: Check the number of unique weights assigned to each fighter per fight
weight_issues <- cleaned_data %>%
  group_by(r_fighter, b_fighter, date) %>%
  summarize(
    unique_r_weights = n_distinct(r_weight_lbs),  # Unique weights for Red fighter
    unique_b_weights = n_distinct(b_weight_lbs),  # Unique weights for Blue fighter
    .groups = "drop"
  )

# Step 2: Create tables showing the frequency of unique weights per fight
red_weight_table <- table(weight_issues$unique_r_weights)
blue_weight_table <- table(weight_issues$unique_b_weights)

# Step 3: Display the tables for review
print("Frequency of Unique Weights for Red Fighters:")
print(red_weight_table)

print("Frequency of Unique Weights for Blue Fighters:")
print(blue_weight_table)

# Step 4: Note that inconsistencies in weights suggest these variables should be removed
print("The above tables demonstrate that r_weight_lbs and b_weight_lbs have zero varation in individual fighters. This is factually untrue and can be verified using fight data from the UFC website. As such, the variables ought to be removed. ")


# Step 5: Remove the problematic weight variables
cleaned_data <- cleaned_data %>%
  dplyr::select(-r_weight_lbs, -b_weight_lbs)
```

## Feature Engineering and Variable Development

  As we have cleaned and standardized the data set, this paper will now attempt to design several new variables, to be used in the prediction models, to try to examine possible gaps of information that are not being considered in the official betting odds. Creating a new variable enables us to potentially determine more information about fighter performance that isn't directly related to the pre-fight statistics or included in the official odds. We would generally not expect models built off of public ally available variables alone to produce gains in accuracy over the odds, as if this information is available, it should lead to arbitrage, with the thus market shifting to the true expected win-probability. These novel variables allow for the potential to reveal new patterns or trends within the existing data.

### a. Style Clustering

  The first feature we will attempt to create and describe are "style" groupings for each fighter. It is often said that in MMA "styles make fights". Oftentimes, rock-paper-scissors type outcomes occur in MMA where one fighter has advantages over another fighter based on their style but disadvantages over others, making transitive property type "A beat B and B beat C, therefore A should beat C" predictions potentially troublesome. There is no objective indicator of a fighter style that currently exists, so one goal of this analysis was to group fighters into styles based on how their fight statistics indicate how they fight. Examples of such variables include how many strikes a given fighter attempts or lands per fight, how many takedowns they attempt and land, or how many wins a fighter has via knockout or submission. These variables provide insight into fighters' offensive and defensive abilities and tendencies, which help to distinguish overall patterns of how they fight. In addition to this, as these variables are calculated for each fight, this allows for the styles of fighters to evolve and change throughout their career.

  We first proceed with k-means clustering, which assumes that the data can be partitioned into distinct, non-overlapping clusters based on the similarity of the observations, with each observation being assigned to the nearest cluster centroid. Variables that seemed indicative of fighting style were manually chosen. From here, red and blue fighter data was merged into a unified set to ensure groupings for both groups were assigned in the same way. This is done to ensure the clustering is based on a complete view of fighters independent of their specific position in any bout. One area this then leaves unaddressed is the potential for fighters to fight differently when being in the red corner vs. the blue corner. The variables were then standardized before clustering to ensure that variables measured on different scales (e.g., strikes vs. control time) contribute equally to the clustering process. Upon testing many different k-values, a value of 6 was deemed best. Values lower than 6 led to the grouping of un-similar fighters, which had the effect of reducing the differences between the clusters. All of the groups were average in all of the metrics of analysis. Meanwhile, values higher than six complicated the number of possible style matchups, while not meaningfully adding to the model's accuracy or interpretability. Using a k-value of 6, the means of key variables were interpreted, and a manual title and description of each fighter group were assigned. For example, cluster 6 was distinguished by its high values for strikes attempted and landed, which is typical of a "Volume Striker", or a fighter who attempts a high output of punches and kicks to control the pace of the fight, win on the judge's scorecards, and give themselves more chances to throw a fight-ending blow.

  As we can see from the "lm_style_match up_test" and "lm_style_test" logistic regression models, both models include all of the style_matchup combinations and styles respectively, alongside the odds of the fight. While style matchups and individual styles may contribute to predicting fight outcomes, the significance of their coefficients is somewhat limited by controlling for betting odds. This implies that betting odds already incorporate at least some of the information about fighting styles, though some nuanced effects may still be present. As a small note, is important to recognize that mirror matchups such as "Balanced vs BalancedWrestler" and "BalancedWrestler vs Balanced" remain in the stylematchup variable, to test for the possibility that a red fighter having a certain matchup is different than a blue fighter. 

```{r, echo=FALSE}
# This code section performs k-means clustering to classify fighter styles and analyzes their impact.
# -----------------------------------------------------------------------------------

# Step 1: Select only style-related variables for clustering
combined_data <- cleaned_data %>%
  dplyr::select(
    b_avg_sig_str_att, b_avg_sig_str_landed, b_avg_td_att, b_avg_td_landed, 
    b_avg_sub_att, b_avg_ctrl_time_seconds, b_wins_by_ko, b_wins_by_submission,
    r_avg_sig_str_att, r_avg_sig_str_landed, r_avg_td_att, r_avg_td_landed, 
    r_avg_sub_att, r_avg_ctrl_time_seconds, r_wins_by_ko, r_wins_by_submission
  ) %>%
  filter(complete.cases(.))  # Keep rows with complete data

# Step 2: Combine data for red and blue fighters into a unified dataset for clustering
unified_data <- cleaned_data %>%
  dplyr::select(
    b_avg_sig_str_att, b_avg_sig_str_landed, b_avg_td_att, b_avg_td_landed, 
    b_avg_sub_att, b_avg_ctrl_time_seconds, b_wins_by_ko, b_wins_by_submission
  ) %>%
  rename_with(~ gsub("^b_", "", .)) %>%  # Standardize column names for blue fighters
  bind_rows(
    cleaned_data %>%
      dplyr::select(
        r_avg_sig_str_att, r_avg_sig_str_landed, r_avg_td_att, r_avg_td_landed, 
        r_avg_sub_att, r_avg_ctrl_time_seconds, r_wins_by_ko, r_wins_by_submission
      ) %>%
      rename_with(~ gsub("^r_", "", .))  # Standardize column names for red fighters
  ) %>%
  filter(complete.cases(.))  # Keep rows with complete data

# Step 3: Standardize the data for clustering
unified_data_scaled <- scale(unified_data)

# Step 4: Perform K-means clustering to classify fighter styles
set.seed(123)  # Ensure reproducibility
kmeans_clusters <- kmeans(unified_data_scaled, centers = 6)  # Using 6 clusters

# Step 5: Assign cluster labels back to red and blue fighters
unified_data$style_cluster <- kmeans_clusters$cluster
cleaned_data$b_style_cluster <- unified_data$style_cluster[1:nrow(cleaned_data)]
cleaned_data$r_style_cluster <- unified_data$style_cluster[(nrow(cleaned_data) + 1):(2 * nrow(cleaned_data))]

# Step 6: Calculate cluster means to analyze style characteristics
cluster_means <- unified_data %>%
  group_by(style_cluster) %>%
  summarise(across(everything(), mean, na.rm = TRUE))
cluster_means %>%
  kable(escape = TRUE,digits = 2, caption = "K-Means Cluster Averages") %>%
   kable_styling(full_width = FALSE, latex_options = "scale_down")

# Step 7: Convert clusters into descriptive categories for red and blue fighters
cleaned_data$b_style_cluster <- recode_factor(as.factor(cleaned_data$b_style_cluster),
                                              `1` = "BalancedWrestler", 
                                              `2` = "PowerStriker", 
                                              `3` = "Grappler", 
                                              `4` = "VolumeWrestler",
                                              `5` = "Balanced", 
                                              `6` = "VolumeStriker")

cleaned_data$r_style_cluster <- recode_factor(as.factor(cleaned_data$r_style_cluster),
                                              `1` = "BalancedWrestler", 
                                              `2` = "PowerStriker", 
                                              `3` = "Grappler", 
                                              `4` = "VolumeWrestler",
                                              `5` = "Balanced", 
                                              `6` = "VolumeStriker")

# Step 8: Create a style matchup variable for regression analysis
cleaned_data$style_matchup <- paste0(cleaned_data$r_style_cluster, "_vs_", cleaned_data$b_style_cluster)

# Verify the distribution of style clusters for red fighters
table(cleaned_data$r_style_cluster)%>%
  kable(escape = TRUE,digits = 2, caption = "Frequency of K-Means Style Clusters") %>%
  kable_styling(full_width = FALSE)

# Step 9: Perform logistic regression to analyze the effect of style matchups
lm_style_matchup_test <- glm(winner ~ r_odds + style_matchup, data = cleaned_data, family = "binomial")
lm_style_test <- glm(winner ~ r_odds + r_style_cluster + b_style_cluster, data = cleaned_data, family = "binomial")



lm_style_matchup_tidy <- tidy(lm_style_matchup_test, conf.int = FALSE)
lm_style_tidy <- tidy(lm_style_test, conf.int = FALSE)

# Print the tidy tables
lm_style_matchup_tidy %>% head(8)%>%
  kable(escape = TRUE,digits = 2, caption = "Logistic Regression: Style Matchup\n(Only the first 8 of 37 rows are displayed") %>%
   kable_styling(full_width = FALSE, latex_options = "scale_down")

lm_style_tidy %>%
  kable(escape = TRUE,digits = 2, caption = "Logistic Regression: Individual Styles") %>%
   kable_styling(full_width = FALSE, latex_options = "scale_down")

```

Descriptions of each of these fight styles have been included in the table below.


```{r, echo =FALSE}
# Create a data frame from the provided table content
style_data <- data.frame(
  Style_Cluster = c(
    "Balanced Wrestler",
    "Power Striker",
    "Grappler",
    "Volume Wrestler",
    "Balanced",
    "Volume Striker"
  ),
  Description = c(
    "Combines striking with wrestling, aiming to control opponents on the ground. Often wins by keeping the opponent down and landing strikes from above.",
    "Focuses on powerful punches and kicks to get a knockout. Strong at striking from a distance but may struggle if taken to the ground.",
    "Aims to bring the opponent to the ground for submissions like chokeholds. Skilled in controlling opponents and winning by forcing them to \"tap out.\"",
    "Keeps a high pace with frequent takedowns and ground control. Wears opponents down with repeated attacks, often winning by decision.",
    "A well-rounded fighter with solid skills in both striking and grappling. Adapts to different opponents and can win by either strikes or grappling.",
    "Overwhelms opponents with lots of punches and kicks, aiming to outscore them rather than knock them out. Often wins by decision."
  ),
  stringsAsFactors = FALSE
)

style_data %>% kable(booktabs = TRUE, escape = TRUE,
      caption = "Style Clusters and Their Descriptions") %>%
  kable_styling(full_width = FALSE, latex_options = "scale_down") %>%
  column_spec(1, width = "8cm") %>%
  column_spec(2, width = "10cm")

```




  The next section of the code also attempts to perform clustering to classify fighters according to their fight style but does so using hierarchical clustering rather than k-means clustering. Hierarchical clustering works by forming a tree-like structure of nested clusters based on a distance metric, without assuming a predefined number of clusters. This is potentially useful as Hierarchical clustering can produce clusters with vastly different shapes and sizes based on the similarity of the data points, whereas k-means clustering tends to create clusters that are more compact and of similar size and shape. This can be seen in the counts of each fight cluster in the final results for each clustering model, with the counts of each style being much more uniform in the k-means model as compared to the hierarchical model. Euclidean distance was used as the metric for measuring similarity between fighters, as it captures straight-line differences across the scaled variables. Additionally, the use of the Wards method minimizes the total in-cluster variance, helping clusters to be more cohesive and similar. To align the results of this clustering with the previous section, a classification rule was drawn to lead to six different groups. Although differing in the specific values and means of fighters in each cluster, the same six labels from k-means clustering appear to be a reasonable fit, which allows for better comparison between the two models. Similar to before, logistic models fit with the hierarchal clustering's style and style matchup variables. The results appear to be mostly similar.

```{r, echo=FALSE}
# This code section performs hierarchical clustering to classify fighter styles and analyzes their impact.
# -----------------------------------------------------------------------------------

# Step 1: Select only style-related variables for clustering
combined_data_hier_clust <- cleaned_data %>%
  dplyr::select(
    b_avg_sig_str_att, b_avg_sig_str_landed, b_avg_td_att, b_avg_td_landed, 
    b_avg_sub_att, b_avg_ctrl_time_seconds, b_wins_by_ko, b_wins_by_submission,
    r_avg_sig_str_att, r_avg_sig_str_landed, r_avg_td_att, r_avg_td_landed, 
    r_avg_sub_att, r_avg_ctrl_time_seconds, r_wins_by_ko, r_wins_by_submission
  ) %>%
  filter(complete.cases(.))  # Retain only rows with complete data

# Step 2: Combine data for red and blue fighters into a unified dataset for clustering
unified_data_hier_clust <- cleaned_data %>%
  dplyr::select(
    b_avg_sig_str_att, b_avg_sig_str_landed, b_avg_td_att, b_avg_td_landed, 
    b_avg_sub_att, b_avg_ctrl_time_seconds, b_wins_by_ko, b_wins_by_submission
  ) %>%
  rename_with(~ gsub("^b_", "", .)) %>%  # Standardize column names for blue fighters
  bind_rows(
    cleaned_data %>%
      dplyr::select(
        r_avg_sig_str_att, r_avg_sig_str_landed, r_avg_td_att, r_avg_td_landed, 
        r_avg_sub_att, r_avg_ctrl_time_seconds, r_wins_by_ko, r_wins_by_submission
      ) %>%
      rename_with(~ gsub("^r_", "", .))  # Standardize column names for red fighters
  ) %>%
  filter(complete.cases(.))  # Retain only rows with complete data

# Step 3: Standardize the data for clustering
unified_data_scaled_hier_clust <- scale(unified_data_hier_clust)

# Step 4: Perform hierarchical clustering using Euclidean distance and Ward's method
dist_matrix_hier_clust <- dist(unified_data_scaled_hier_clust, method = "euclidean")
hclust_result_hier_clust <- hclust(dist_matrix_hier_clust, method = "ward.D2")

# Step 5: Cut the dendrogram into 6 clusters
num_clusters_hier_clust <- 6
hclust_clusters_hier_clust <- cutree(hclust_result_hier_clust, k = num_clusters_hier_clust)

# Step 6: Assign cluster labels back to red and blue fighters
unified_data_hier_clust$style_cluster_hier_clust <- hclust_clusters_hier_clust
cleaned_data$b_style_cluster_hier_clust <- unified_data_hier_clust$style_cluster_hier_clust[1:nrow(cleaned_data)]
cleaned_data$r_style_cluster_hier_clust <- unified_data_hier_clust$style_cluster_hier_clust[(nrow(cleaned_data) + 1):(2 * nrow(cleaned_data))]

# Step 7: Calculate means for each cluster to understand style characteristics
cluster_means_hier_clust <- unified_data_hier_clust %>%
  group_by(style_cluster_hier_clust) %>%
  summarise(across(everything(), mean, na.rm = TRUE))

# View cluster means for better understanding of style differences
cluster_means_hier_clust%>%
  kable(escape = TRUE,digits = 2, caption = "Hiercarchal Cluster Group Means") %>%
   kable_styling(full_width = FALSE, latex_options = "scale_down")
      
      
# Step 8: Recode clusters into descriptive categories for interpretability
cleaned_data$b_style_cluster_hier_clust <- recode_factor(as.factor(cleaned_data$b_style_cluster_hier_clust), 
                                                         `1` = "Balanced", 
                                                         `2` = "VolumeWrestler", 
                                                         `3` = "PowerStriker", 
                                                         `4` = "BalancedWrestler",
                                                         `5` = "VolumeStriker", 
                                                         `6` = "Grappler")

cleaned_data$r_style_cluster_hier_clust <- recode_factor(as.factor(cleaned_data$r_style_cluster_hier_clust), 
                                                         `1` = "Balanced", 
                                                         `2` = "VolumeWrestler", 
                                                         `3` = "PowerStriker", 
                                                         `4` = "BalancedWrestler",
                                                         `5` = "VolumeStriker", 
                                                         `6` = "Grappler")

# Step 9: Create a style matchup column
cleaned_data$style_matchup_hier_clust <- paste0(cleaned_data$r_style_cluster_hier_clust, "_vs_", cleaned_data$b_style_cluster_hier_clust)

# Step 10: View the distribution of red fighter styles
table(cleaned_data$r_style_cluster_hier_clust) %>%
  kable(escape = TRUE,digits = 2, caption = "Hiercarchical Cluster Group Counts") %>%
   kable_styling(full_width = FALSE)

# Step 11: Perform logistic regression for style matchups
lm_style_matchup_hier_clust_test <- glm(winner ~ r_odds + style_matchup_hier_clust , 
                                   data = cleaned_data, family = "binomial")




# Step 12: Perform logistic regression for style-based clusters
lm_style_hier_clust_test <- glm(winner ~ r_odds + r_style_cluster_hier_clust + b_style_cluster_hier_clust, 
                           data = cleaned_data, family = "binomial")


lm_style_matchup_hier_clust_tidy <- tidy(lm_style_matchup_hier_clust_test, conf.int = FALSE)
lm_style_hier_clust_test_tidy <- tidy(lm_style_hier_clust_test, conf.int = FALSE)

lm_style_matchup_hier_clust_tidy %>%  head(8) %>%
  kable(escape = TRUE,digits = 2, caption = "Logistic Regression: Hierarchical Individual Styles\n(Only the first 8/37 rows are displayed)"
 ) %>%
   kable_styling(full_width = FALSE, latex_options = "scale_down")

lm_style_hier_clust_test_tidy %>%
  kable(escape = TRUE,digits = 2, caption = "Logistic Regression: Hierarchical Individual Styles") %>%
   kable_styling(full_width = FALSE, latex_options = "scale_down")


```

  To simplify our analysis and make it less redundant going forward, we will now compare the k-means and hierarchical clustering models to determine which is more effective as a clustering method. First, note the cross-tabulated assignment tables. As we can see, there is interesting an agreement of 59% for red fighters and 61% when clustering blue fighters. The agreement rates between k-means indicate a moderate amount of alignment between the two, suggesting that while both methods capture similar trends in fighter styles while differ for a large portion of the data. As evident from the bar plot, it is clear that the k-means model is assigning significantly more fighters to the BalancedWrestler group, and way less to the VolumeWrestler group, relative to the hierarchical model. Finally, AIC scores are computed for each model. The k-means model is determined to have a lower AIC value relative to the hierarchical model, indicating that it has a better balance between fit and complexity. While k-means clustering offers simplicity and a lower AIC score, its assumption of compact elliptical clusters may fail to determine the true relationship and shape of groupings. Nonetheless, the k-means model's style and style_matchup variables are what is used going forward for the rest of the paper.



```{r, echo=FALSE} 
# This code section compares the k-means and hierarchical clustering models by evaluating their ability 
# to classify fighter styles and their predictive power in logistic regression models. Ultimately, the 
# k-means model is chosen to continue with. 
# -----------------------------------------------------------------------------------

# Step 1: Evaluate the cluster assignments
# Cross-tabulate k-means and hierarchical cluster assignments for both red and blue fighters
kmeans_vs_hier_red <- table(cleaned_data$r_style_cluster, cleaned_data$r_style_cluster_hier_clust)
kmeans_vs_hier_blue <- table(cleaned_data$b_style_cluster, cleaned_data$b_style_cluster_hier_clust)

# Step 1.5: Subset both matrices to include only the common columns
common_columns <- intersect(rownames(kmeans_vs_hier_red), colnames(kmeans_vs_hier_blue))
kmeans_vs_hier_red <- kmeans_vs_hier_red[common_columns, common_columns, drop = FALSE]
kmeans_vs_hier_blue <- kmeans_vs_hier_blue[common_columns, common_columns, drop = FALSE]

# Step 2: Calculate the agreement rate between clustering methods
agreement_rate_red <- sum(diag(kmeans_vs_hier_red)) / sum(kmeans_vs_hier_red)
agreement_rate_blue <- sum(diag(kmeans_vs_hier_blue)) / sum(kmeans_vs_hier_blue)

# Step 3: Compare regression models using AIC
# Extract AIC values from the logistic regression models
aic_kmeans_style_matchup <- AIC(lm_style_matchup_test)
aic_hier_style_matchup <- AIC(lm_style_matchup_hier_clust_test)

aic_kmeans_styles <- AIC(lm_style_test)
aic_hier_styles <- AIC(lm_style_hier_clust_test)

# Create a comparison table for AIC values
aic_comparison <- data.frame(
  Model = c("K-Means Style Matchup", "Hierarchical Style Matchup", "K-Means Styles", "Hierarchical Styles"),
  AIC = c(aic_kmeans_style_matchup, aic_hier_style_matchup, aic_kmeans_styles, aic_hier_styles)
)

# Determine which models perform better based on AIC
style_matchup_result <- if (aic_kmeans_style_matchup < aic_hier_style_matchup) {
  "K-Means performs better (lower AIC)"
} else {
  "Hierarchical performs better (lower AIC)"
}

styles_result <- if (aic_kmeans_styles < aic_hier_styles) {
  "K-Means performs better (lower AIC)"
} else {
  "Hierarchical performs better (lower AIC)"
}

# Create a single summary table of key print statements and values
results_summary <- data.frame(
  Description = c(
    "Red Fighter: K-Means vs Hierarchical Clusters",
    "Blue Fighter: K-Means vs Hierarchical Clusters",
    "Agreement Rate for Red Fighters",
    "Agreement Rate for Blue Fighters",
    "AIC Comparison Between Models",
    "Model Performance (Style Matchup)",
    "Model Performance (Styles)"
  ),
  Value = c(
    "See cluster tables below",
    "See cluster tables below",
    round(agreement_rate_red, 2),
    round(agreement_rate_blue, 2),
    "See the AIC Comparison Table",
    style_matchup_result,
    styles_result
  ),
  stringsAsFactors = FALSE
)

# Print the results summary table
results_summary %>%
  kable(
    digits = 2, 
    caption = "K-Means vs Hierarchical Clustering Comparisons",
    escape = TRUE
  ) %>%
  kable_styling(full_width = FALSE)

# Print the AIC comparison table
aic_comparison%>% 
  kable(
    digits = 2, 
    caption = "K-Means vs Hierarchical Clustering AIC",
    escape = TRUE
  ) %>%
  kable_styling(full_width = FALSE)

# Step 5: Visualize cluster comparisons
kmeans_cluster_table <- table(cleaned_data$r_style_cluster)
kmeans_cluster_table <- kmeans_cluster_table[order(names(kmeans_cluster_table))]

hier_cluster_table <- table(cleaned_data$r_style_cluster_hier_clust)
hier_cluster_table <- hier_cluster_table[order(names(hier_cluster_table))]

combined_clusters <- rbind(kmeans_cluster_table, hier_cluster_table)
rownames(combined_clusters) <- c("K-Means", "Hierarchical")

bar_positions <- barplot(
  combined_clusters,
  beside = TRUE,  
  col = c("blue", "red"),  
  ylim = c(0, max(combined_clusters) * 1.2),  
  main = "Red Fighter Clusters Comparison",
  ylab = "Frequency",
  xlab = "Cluster",
  names.arg = colnames(combined_clusters),  
  cex.names = 0.6 
)

legend(
  "topright",
  legend = rownames(combined_clusters),
  fill = c("blue", "red"),
  title = "Clustering Method",
  cex = 0.7
)


```

### b. Activity/Finishing Scores

  The next section of this paper will involve the creation of two scores. The first score we will construct is an "activity score", which is attempting to measure how "active" a fighter is, or how many offensive actions that they attempt to do per fight. All of the "attempt" variables are standardized, and then the sum is taken. This results in a score that measures overall activness that a fighter has had in their fights up to that point in their career. As we can see, there is a large amount of variation in these scores based on the below plots. From lm_activity_test, it is clear that, even when taking into account the betting odds, high values of r_activity_score and low b_activity_score values both significantly and positively contribute to the chance of a red victory. This suggests that the betting markets aren't fully taking the activity of fighters into account when making odds for a fight. One potential area of concern is the possibility that fighter's activity linearly decreases with age. As the activity score functions as a rolling average of all of their fights, this may bias the true value if fighters are no longer as active as they were when begging their ufc career. The plot suggests a negative correlation for both red and blue fighters, but there is a significant amount of variation and noise where most of the data is clustered. As such, we will move forward with using these variables as initially planned.

```{r, echo=FALSE}
# This code section calculates an activity metric for fighters based on volume/activity-related variables. 
# It standardizes the data, computes a composite score, and evaluates its predictive power for fight outcomes.
# -----------------------------------------------------------------------------------

# Step 1: Select volume/activity-related variables for both Blue and Red fighters
activity_data <- cleaned_data %>%
  dplyr::select(
    b_avg_sig_str_att, b_avg_td_att, b_avg_sub_att,
    r_avg_sig_str_att, r_avg_td_att, r_avg_sub_att
  )

# Step 2: Combine data for both Blue and Red fighters into a unified dataset for activity metric calculation
unified_activity_data <- cleaned_data %>%
  dplyr::select(
    b_avg_sig_str_att, b_avg_td_att, b_avg_sub_att, b_avg_ctrl_time_seconds
  ) %>%
  rename_with(~ gsub("^b_", "", .), everything()) %>%
  bind_rows(
    cleaned_data %>%
      dplyr::select(
        r_avg_sig_str_att, r_avg_td_att, r_avg_sub_att, r_avg_ctrl_time_seconds
      ) %>%
      rename_with(~ gsub("^r_", "", .), everything())
  )

# Step 3: Standardize the unified dataset
unified_activity_scaled <- scale(unified_activity_data)

# Step 4: Calculate a composite activity score by summing the standardized values
unified_activity_data$activity_score <- rowSums(unified_activity_scaled)

# Step 5: Assign the calculated activity score back to both Blue and Red fighters in the original dataset
cleaned_data$b_activity_score <- unified_activity_data$activity_score[1:nrow(cleaned_data)]
cleaned_data$r_activity_score <- unified_activity_data$activity_score[(nrow(cleaned_data) + 1):(2 * nrow(cleaned_data))]

# Step 6: Verify the new activity score columns for Blue and Red fighters
blue_summary <- summary(cleaned_data$b_activity_score)
red_summary <- summary(cleaned_data$r_activity_score)

combined_summary <- data.frame(
  Statistic = names(blue_summary),
  Blue = as.numeric(blue_summary),
  Red = as.numeric(red_summary),
  stringsAsFactors = FALSE
)

combined_summary %>% 
  kable(
    digits = 2, 
    caption = "Activity Score Summary by Color",
    escape = TRUE
  ) %>%
  kable_styling(full_width = FALSE)

# Step 7: Perform logistic regression to evaluate the impact of activity scores on fight outcomes
lm_activity_test <- glm(winner ~ r_odds + b_activity_score + r_activity_score, data = cleaned_data, family = "binomial")

lm_activity_test%>% tidy(conf.int=TRUE) %>% 
  kable(
    digits = 2, 
    caption = "Activity Score Logistic Regression",
    escape = TRUE
  ) %>%
  kable_styling(full_width = FALSE)


# Step 8:  Look at age's relationship with activity score 

# Filter data to include only rows with fighters' ages between 21 and 40
filtered_data <- cleaned_data %>%
  filter(r_age >= 21 & r_age <= 40 & b_age >= 21 & b_age <= 40)

# Compute average activity scores by age for both red and blue fighters
activity_by_age <- filtered_data %>%
  group_by(r_age) %>%
  summarise(
    avg_activity_score_r = mean(r_activity_score, na.rm = TRUE)
  ) %>%
  rename(age = r_age) %>%
  bind_rows(
    filtered_data %>%
      group_by(b_age) %>%
      summarise(
        avg_activity_score_b = mean(b_activity_score, na.rm = TRUE)
      ) %>%
      rename(age = b_age)
  ) %>%
  pivot_longer(cols = starts_with("avg_activity_score"), names_to = "fighter", values_to = "avg_activity_score")

# Create the plot
ggplot(activity_by_age, aes(x = age, y = avg_activity_score, color = fighter)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    title = "Average Activity Score by Age (21-40)",
    x = "Age",
    y = "Average Activity Score",
    color = "Fighter"
  ) +
  theme_minimal() +
  scale_color_manual(labels = c("Blue Fighter", "Red Fighter"), values = c("blue", "red"))

```

  Next, we will construct a metric attempting to measure the "finishing ability" of a fighter, or their ability to finish and win a fight based on a knockout, technical knockout, or submission. Finishing a fight, allows for a more consistent guarantee of victory, as going to the judge's for a decision sometimes results in fighters losing when they expected to win. Variables are chosen to measure this finishing ability, while not overlapping with the activity score metric. Finishing rates per fight, as well as per round, are computed, standardized, and then summed. As we can see from the plots, the finishing score metrics also remain relatively symmetric about the expected value of zero. Meanwhile, the results from the lm_finishing_test logistic regression model are quite uprising. This linear model suggests that blue fighters having a higher finishing ability raises the probability of red winning when the odds are taken into account. This suggests that bettors are overbetting on blue fighters with high finishing ability. Notably, these variables are no longer significant when looking at the 2+ prior fights for each opponent subset. While the finishing score appears to be a useful metric, it does not account for context in the quality of opponents or situation factors during fights. Incorporating opponent-quality-adjusted metrics could enhance the accuracy of the score in future analyses.


```{r, echo=FALSE}
# This code section calculates a finishing ability score for fighters, incorporating metrics like KO, submission, 
# and doctor stoppage wins per fight and per win. These scores aim to measure finishing ability and evaluate its impact 
# on fight outcomes using regression analysis.
# -----------------------------------------------------------------------------------

# Step 1: Select and normalize finishing-related variables (excluding knockdowns)
finishing_data <- cleaned_data %>%
  mutate(
    # Blue fighter finishing rate per round
    b_finishing_per_round = ifelse(!is.na(b_total_rounds_fought) & b_total_rounds_fought > 0,
                                   (b_wins_by_ko + b_wins_by_submission + b_wins_by_tko_doctor_stoppage) / b_total_rounds_fought,
                                   NA),
    
    # Red fighter finishing rate per round
    r_finishing_per_round = ifelse(!is.na(r_total_rounds_fought) & r_total_rounds_fought > 0,
                                   (r_wins_by_ko + r_wins_by_submission + r_wins_by_tko_doctor_stoppage) / r_total_rounds_fought,
                                   NA)
  )


# Step 2: Calculate per fight and per win metrics for Blue and Red fighters
finishing_data <- finishing_data %>%
  mutate(
    # Blue fighter metrics per fight and per win
    b_finishing_per_fight = ifelse(b_total_rounds_fought > 0,
                                   (b_wins_by_ko + b_wins_by_submission + b_wins_by_tko_doctor_stoppage) / b_total_rounds_fought,
                                   NA),
    b_finishing_per_win = ifelse(b_wins > 0,
                                 (b_wins_by_ko + b_wins_by_submission + b_wins_by_tko_doctor_stoppage) / b_wins,
                                 NA),
    
    # Red fighter metrics per fight and per win
    r_finishing_per_fight = ifelse(r_total_rounds_fought > 0,
                                   (r_wins_by_ko + r_wins_by_submission + r_wins_by_tko_doctor_stoppage) / r_total_rounds_fought,
                                   NA),
    r_finishing_per_win = ifelse(r_wins > 0,
                                 (r_wins_by_ko + r_wins_by_submission + r_wins_by_tko_doctor_stoppage) / r_wins,
                                 NA)
  )

# Step 3: Combine finishing metrics for Blue and Red fighters into a unified dataset
unified_finishing_data <- finishing_data %>%
  dplyr::select(b_finishing_per_fight, b_finishing_per_win) %>%
  rename_with(~ gsub("^b_", "", .)) %>%
  bind_rows(
    finishing_data %>%
      dplyr::select(r_finishing_per_fight, r_finishing_per_win) %>%
      rename_with(~ gsub("^r_", "", .))
  )

# Step 4: Standardize the unified dataset
unified_finishing_scaled <- scale(unified_finishing_data, center = TRUE, scale = TRUE)

# Step 5: Calculate a composite finishing score by summing the scaled values
unified_finishing_data$enhanced_finishing_score <- rowSums(unified_finishing_scaled, na.rm = TRUE)

# Step 6: Assign the finishing score back to Blue and Red fighters
cleaned_data$b_finishing_score <- unified_finishing_data$enhanced_finishing_score[1:nrow(cleaned_data)]
cleaned_data$r_finishing_score <- unified_finishing_data$enhanced_finishing_score[(nrow(cleaned_data) + 1):(2 * nrow(cleaned_data))]

# Step 7: Verify the finishing score columns
blue_summary <- summary(cleaned_data$b_finishing_score)
red_summary <- summary(cleaned_data$r_finishing_score)

combined_summary <- data.frame(
  Statistic = names(blue_summary),
  Blue = as.numeric(blue_summary),
  Red = as.numeric(red_summary),
  stringsAsFactors = FALSE
)
combined_summary %>%
  kable(digits = 2, caption = "Summary of Blue and Red Fighters' Finishing Scores") %>%
  kable_styling(full_width = FALSE)

# Step 8: Make the Logistic Regression



lm_finishing_test <- glm(winner ~ r_odds+r_finishing_score+b_finishing_score, family="binomial", data=cleaned_data)
lm_finishing_test%>% tidy(conf.int=TRUE) %>% 
  kable(
    digits = 2, 
    caption = "Finishing Score Logistic Regression",
    escape = TRUE
  ) %>%
  kable_styling(full_width = FALSE)

#

```

### c. Other Variables

  Next, we will construct a variable to measure the number of prior fights each fighter has had at the weight class the fight in question is contested. This variable aims to measure a fighter's familiarity and general experience within a specific weight class, which could influence their adaptability and comfort during the fight. We also create a binary variable to test whether or not the location of the given fight is in Las Vegas, which makes up the plurality of locations of UFC fights. This could allow us to test for significant advantages or disadvantages to fighters, potentially due to factors such as travel, crowd support, or familiarity with the venue. From lm_prior_fights_test, we can observe neither prior fights metric is statistically significant when the odds are already taken into account. Meanwhile, the lm_vegas_test model suggests that fights occurring in Las Vegas as compared to other places are not significant, but it should be noted that this point estimate is negative and its p-value is somewhat close to the 0.05 threshold, approaching borderline significance.

 
```{r, include=FALSE}
# This code section creates a "total prior fights at fight's weight class" variable for both red and blue fighters. It also creates a binary variable to track whether or not the fight is occuring within Vegas or not. 
# -----------------------------------------------------------------------------------


# Combine r_fighter and b_fighter into a single column with weight class and date
all_fights <- cleaned_data %>%
  dplyr::select(r_fighter, weight_class, date) %>%
  rename(fighter = r_fighter) %>%
  bind_rows(
    cleaned_data %>%
      dplyr::select(b_fighter, weight_class, date) %>%
      rename(fighter = b_fighter)
  ) %>%
  arrange(fighter, weight_class, date)

# Add a column for the total number of prior fights within each weight class
all_fights <- all_fights %>%
  group_by(fighter, weight_class) %>%
  mutate(prior_fights = row_number() - 1)

# Join prior fight counts back into the main dataset for red and blue fighters
cleaned_data <- cleaned_data %>%
  left_join(all_fights, by = c("r_fighter" = "fighter", "weight_class", "date")) %>%
  rename(r_prior_fights = prior_fights) %>%
  left_join(all_fights, by = c("b_fighter" = "fighter", "weight_class", "date")) %>%
  rename(b_prior_fights = prior_fights)

# Verify the updated dataset
# head(cleaned_data %>% select(r_fighter, b_fighter, weight_class, date, r_prior_fights, b_prior_fights)) %>% arrange(r_fighter, date)

lm_prior_fights_test <- glm(winner ~ r_odds+r_prior_fights+b_prior_fights, data=cleaned_data, family="binomial")
summary(lm_prior_fights_test)

# Add Vegas indicator variable and model its impact
cleaned_data <- cleaned_data %>%
  mutate(vegas = location == "Las Vegas, Nevada, USA")

lm_vegas_test <- glm(winner ~ r_odds+vegas, data=cleaned_data, family="binomial")
summary(lm_vegas_test)

```

  Next, a unified "ranking variable" is added to the data. In each weight class fighters are ranked beginning with champion (or rank 0), followed by the next top 15 competitors at that weight class. In the original data frames, each of these weight rankings is listed in a separate column for each. The below code takes the average of all of a fighter's rankings in all of the weight classes they are ranked in. However, as only the top 15 contenders are ranked in any division, most fighters are unranked, which limits the size and power of this variable as a predictor. The lm_ranking_test model clearly shows a lack of significance for either ranking metric, which can be interpreted as the betting odds already taking this information into account. The overlapping histograms of red and blue fighter rankings align with our expectations for the distribution of rankings between the two groups, with red generally having higher ranking values, and also with the expectation that most fighters are unranked.

```{r, include=FALSE}
# This code section creates and analyzes a unified ranking variable for fighters.
# -----------------------------------------------------------------------------------

# Step 1: Identify columns containing "rank" for red (r_) and blue (b_) fighters
r_rank_columns <- grep("^r_.*rank", colnames(cleaned_data), value = TRUE)
b_rank_columns <- grep("^b_.*rank", colnames(cleaned_data), value = TRUE)

# Step 2: Compute average rank for red fighters
cleaned_data$r_Ranking <- apply(cleaned_data[, r_rank_columns], 1, function(x) {
  numeric_ranks <- suppressWarnings(as.numeric(x))  # Convert to numeric, handle NAs
  non_na_ranks <- numeric_ranks[!is.na(numeric_ranks)]  # Remove NAs
  if (length(non_na_ranks) > 0) mean(non_na_ranks) else NA  # Return mean or NA
})

# Step 3: Compute average rank for blue fighters
cleaned_data$b_Ranking <- apply(cleaned_data[, b_rank_columns], 1, function(x) {
  numeric_ranks <- suppressWarnings(as.numeric(x))  # Convert to numeric, handle NAs
  non_na_ranks <- numeric_ranks[!is.na(numeric_ranks)]  # Remove NAs
  if (length(non_na_ranks) > 0) mean(non_na_ranks) else NA  # Return mean or NA
})

# Step 4: Verify the new rank variables
table(cleaned_data$r_Ranking, useNA = "always")  # Include NA values in summary
table(cleaned_data$b_Ranking, useNA = "always")  # Include NA values in summary

# Step 5: Plot overlapping histograms of red and blue fighter rankings
hist(cleaned_data$r_Ranking, col = rgb(1, 0, 0, 0.5), xlim = c(0, 15), 
     main = "Overlapping Histograms of Fighter Rankings",
     xlab = "Ranking", ylab = "Frequency", breaks = 15)
hist(cleaned_data$b_Ranking, col = rgb(0, 0, 1, 0.5), add = TRUE, breaks = 15)

# Add legend to the histogram
legend("topright", legend = c("Red Fighter Rankings", "Blue Fighter Rankings"), 
       fill = c(rgb(1, 0, 0, 0.5), rgb(0, 0, 1, 0.5)))


lm_ranking_test <- glm(winner ~ r_odds+r_Ranking+b_Ranking,family="binomial", data=cleaned_data)
summary(lm_ranking_test)
```



  Factor analysis was then applied to uncover latent variables in the dataset, focusing on performance metrics for red and blue fighters. Weight-class-specific ranking variables were removed to avoid confounding effects, and numeric variables for both red and blue fighters were standardized to ensure that differences in scale or units did not disproportionately influence the factor analysis. A scree plot of eigenvalues was used to determine the number of factors, with 4 factors chosen due to a noticeable drop in explanatory power beginning with the 5th factor. The results of Mardia's test for multivariate normality indicate that the data deviates significantly from multivariate normality, as evidenced by the high skewness and kurtosis statistics and p-values of 0 for both components of the test. Despite this, the analysis will proceed with regular factor analysis to explore the latent factor structure, acknowledging the potential limitations posed by the violation of normality assumptions.

  The factor scores for red and blue fighters were calculated, and the loadings revealed considerable overlap between the red and blue factors, indicating that similar patterns of underlying skills and characteristics are present across both groups. The logistic regression model (lm_factors_test) showed that for both red and blue fighters, Factor 3 was significant at the alpha=0.05 level when betting odds were included. Red Factor 3 is dominated by variables involving ground control, such as ground position control time, strikes attempted on the ground, and total strikes attempted, whereas Blue Factor 3 captures similar variables but focuses on strikes attempted and landed on the ground. The two factors provide complementary insights rather than redundancy, with Blue Factor 3 reflecting more offensive measures, while Red Factor 3 emphasizes control and positional dominance.

  The lack of significance for the other factors suggests that their information is already accounted for by the betting odds and does not contribute additional predictive power. This result highlights that the betting odds capture much of the fighters' overall skill and performance metrics. While the factors provide a simplified summary of fighter characteristics, the reliance on selected numeric variables and the exclusion of non-numeric data may limit their robustness to new data and general interpretability.


```{r, echo=FALSE}
# This section performs Factor Analysis to reduce dimensionality for both Red and Blue fighters.
# It includes data preprocessing, scaling, handling missing values, and generating factor scores.
# -----------------------------------------------------------------------------------


# Remove ranking columns from the dataset
cleaned_data_factor <- cleaned_data %>%
  dplyr::select(-all_of(r_rank_columns), -all_of(b_rank_columns))

# Subset data for Blue fighters
blue_data <- cleaned_data_factor %>% 
  dplyr::select(starts_with("b_"))

# Subset data for Red fighters
red_data <- cleaned_data_factor %>% 
  dplyr::select(starts_with("r_")) %>% 
  dplyr::select(-r_fighter)

# Select numeric columns for Blue and Red fighters
blue_data_numeric <- blue_data %>% 
  dplyr::select(where(is.numeric))

red_data_numeric <- red_data %>% 
  dplyr::select(where(is.numeric))


# Scale data for Blue fighters
blue_scaled <- scale(blue_data_numeric, center = TRUE, scale = TRUE)

# Scale data for Red fighters
red_scaled <- scale(red_data_numeric, center = TRUE, scale = TRUE)


# Remove columns with missing or infinite values
blue_scaled_clean <- blue_scaled[, colSums(is.na(blue_scaled)) == 0 & colSums(is.infinite(blue_scaled)) == 0]


# Calculate the eigenvalues of the correlation matrix
eigenvalues <- eigen(cor(blue_scaled_clean))$values  # Use the scaled data for blue fighters

# Create a scree plot
plot(
  eigenvalues,
  type = "b",  # Points and lines
  pch = 19,    # Solid circle points
  xlab = "Factor Number",
  ylab = "Eigenvalue",
  main = "Scree Plot for Factor Analysis",
  col = "blue"
)

# Add a horizontal line at eigenvalue = 1 for Kaiser criterion
abline(h = 1, col = "red", lty = 2)

# Handle missing values in scaled data
# Identify variables with missing values in blue_scaled
blue_missing_counts <- colSums(is.na(blue_scaled))
# print("Missing values in Blue scaled data:")
# print(blue_missing_counts)

# Identify variables with missing values in red_scaled
red_missing_counts <- colSums(is.na(red_scaled))
# print("Missing values in Red scaled data:")
# print(red_missing_counts)

# Drop columns with missing values in Blue scaled data
blue_scaled <- blue_scaled[, colSums(is.na(blue_scaled)) == 0]

# Drop columns with missing values in Red scaled data
red_scaled <- red_scaled[, colSums(is.na(red_scaled)) == 0]

# Perform Mardia's test for multivariate normality for Blue and Red fighter data
blue_mardia <- mvn(data = as.data.frame(blue_scaled), mvnTest = "mardia")
red_mardia <- mvn(data = as.data.frame(red_scaled), mvnTest = "mardia")

# Print the results of Mardia's test
blue_mardia$multivariateNormality  %>%
  kable(escape = TRUE,digits = 2, caption = "Mardia's Test Results for Blue Fighters") %>%
  kable_styling(full_width = FALSE)

red_mardia$multivariateNormality %>%
  kable(escape = TRUE,digits = 2, caption = "Mardia's Test Results for Red Fighters") %>%
  kable_styling(full_width = FALSE)


# Perform Factor Analysis for Blue fighters
blue_fa <- fa(blue_scaled, nfactors = 4, rotate = "varimax")  # Adjusted nfactors based on scree plot 

# Extract factor scores for Blue fighters
blue_factor_scores <- as.data.frame(factor.scores(blue_scaled, blue_fa)$scores)

# Rename factor scores for clarity
colnames(blue_factor_scores) <- c("b_factor1", "b_factor2", "b_factor3", "b_factor4")  

# Perform Factor Analysis for Red fighters
red_fa <- fa(red_scaled, nfactors = 4, rotate = "varimax")  # Adjust nfactors based on scree plot or domain knowledge

# Extract factor scores for Red fighters
red_factor_scores <- as.data.frame(factor.scores(red_scaled, red_fa)$scores)

# Rename factor scores for clarity
blue_factor_scores <- blue_factor_scores %>%
  rename_with(~ c("b_factor1_StrikingAbility",
                  "b_factor2_ExperienceAndResilience",
                  "b_factor3_GrapplingAbility",
                  "b_factor4_OpponentGrapplingCaliber"))
red_factor_scores <- red_factor_scores %>%
  rename_with(~ c("r_factor1_StrikingAbility",
                  "r_factor2_ExperienceAndResilience",
                  "r_factor3_GrapplingAbility",
                  "r_factor4_OpponentGrapplingCaliber"))



# View the dataset with factor scores
# print("Dataset with Factor Scores (Head):")
# head(cleaned_data_with_factors)



# Get the factor loadings for both Red and Blue fighters
red_loadings <- red_fa$loadings
blue_loadings <- blue_fa$loadings

# Extract the top variables for each factor
top_red_variables <- get_top_variables(red_loadings, num_top = 5)
top_blue_variables <- get_top_variables(blue_loadings, num_top = 5)

# Combine results into a data frame for better visualization
red_table <- data.frame(
  Factor = paste0("Red_Factor", 1:ncol(red_loadings)),
  Top_Variables = apply(top_red_variables, 2, paste, collapse = ", ")
)

blue_table <- data.frame(
  Factor = paste0("Blue_Factor", 1:ncol(blue_loadings)),
  Top_Variables = apply(top_blue_variables, 2, paste, collapse = ", ")
)

# Print the tables
red_table %>%
  kable(escape = TRUE,digits = 2, caption = "Top Variables for Red Factors") %>%
   kable_styling(full_width = FALSE, latex_options = "scale_down")


blue_table %>%
  kable(escape = TRUE,digits = 2, caption = "Top Variables for Blue Factors") %>%
   kable_styling(full_width = FALSE, latex_options = "scale_down")





# Combine factor scores for Red and Blue fighters into the cleaned_data dataset
cleaned_data <- cleaned_data %>%
  bind_cols(blue_factor_scores) %>%
  bind_cols(red_factor_scores)

# Verify the updated dataset
# cat("Updated cleaned_data with factor scores:\n")
# print(head(cleaned_data))

lm_factors_test <- glm(winner ~ r_odds + 
                        r_factor1_StrikingAbility + 
                        r_factor2_ExperienceAndResilience + 
                        r_factor3_GrapplingAbility + 
                        r_factor4_OpponentGrapplingCaliber + 
                        b_factor1_StrikingAbility + 
                        b_factor2_ExperienceAndResilience + 
                        b_factor3_GrapplingAbility + 
                        b_factor4_OpponentGrapplingCaliber, 
                      data = cleaned_data, 
                      family = "binomial")

# Display the summary of the model
lm_factors_test %>% tidy(conf.int = FALSE) %>%
  kable(escape = TRUE,digits = 2, caption = "Logistic Model for Red and Blue Factors") %>%
   kable_styling(full_width = FALSE, latex_options = "scale_down")

```

## Exploratory Data Analysis

  Finally, we will conclude this section by conducting some exploratory data analysis. Before doing so, we will print a table describing all of the major variables involved in the exploration and analysis.


```{r,echo=FALSE}
table_data <- data.frame(
  Variable = c(
    "r_", 
    "b_", 
    "odds", 
    "activity_score", 
    "finishing_score", 
    "age_over_34", 
    "prior_fights", 
    "Ranking", 
    "total_rounds_fought", 
    "wins", 
    "height_cms", 
    "reach_cms", 
    "current_win_streak", 
    "current_lose_streak", 
    "avg_sig_str_landed", 
    "avg_td_pct", 
    "wins_by_ko", 
    "wins_by_submission", 
    "wins_by_tko_doctor_stoppage", 
    "factor1_StrikingAbility", 
    "factor2_ExperienceAndResilience", 
    "factor3_GrapplingAbility", 
    "factor4_OpponentGrapplingCaliber", 
    "style_matchup", 
    "style_cluster", 
    "avg_sig_str_att", 
    "avg_sig_str_landed", 
    "avg_td_att", 
    "avg_td_landed", 
    "avg_sub_att", 
    "avg_ctrl_time_seconds", 
    "vegas"
  ),
  Definition = c(
    "Prefix indicating variables specific to the red fighter",
    "Prefix indicating variables specific to the blue fighter",
    "Implied probability of winning according to betting market",
    "Activity score for the fighter",
    "Finishing score for the fighter",
    "Indicator if the fighter is over 34 years old",
    "Number of prior fights for the fighter in the weight class",
    "Average rank of the fighter across weight classes",
    "Total rounds fought by the fighter",
    "Total wins by the fighter",
    "Height of the fighter (in cm)",
    "Reach/wingspan of the fighter (in cm)",
    "Current win streak of the fighter",
    "Current losing streak of the fighter",
    "Average significant strikes landed per fight",
    "Takedown percentage for the fighter",
    "Wins by knockout for the fighter",
    "Wins by submission for the fighter",
    "Wins by TKO/doctor stoppage for the fighter",
    "First latent factor, representing striking ability",
    "Second latent factor, representing experience and resilience",
    "Third latent factor, representing grappling ability",
    "Fourth latent factor, representing opponent grappling caliber",
    "Style matchup between the red and blue fighters",
    "Cluster label indicating fighting style classification",
    "Average significant strikes attempted per fight",
    "Average significant strikes landed per fight",
    "Average takedowns attempted per fight",
    "Average takedowns landed per fight",
    "Average submission attempts per fight",
    "Average control time in seconds per fight",
    "Indicator if the fight occurred in Las Vegas, Nevada"
  ),
  stringsAsFactors = FALSE
)

table_data %>% kable(escape=TRUE, booktabs = TRUE, 
      caption = "Variable Definitions") %>%
  kable_styling(full_width = FALSE, latex_options = "scale_down") %>%
  # Specify column widths to allow wrapping
  column_spec(1, width = "8cm") %>%
  column_spec(2, width = "10cm")

```



  First, we display the means for several key variables, some of which were constructed in this section. From the summary statistics, it is evident that red fighters generally have slightly higher average activity and finishing scores compared to blue fighters, which may contribute to their tendency to be favored in betting odds. Next, we look at the distribution of ages of fighters, which is fairly symmetric and mostly overlapping for red and blue fighters. We also take a look at the distribution of both r_odds and b_odds, from which it is clear that red fighters are generally the favorite. We also can see the win rate for red fighters per weight division, with the only major note being that the Women's featherweight division suffers from an extremely small sample size of fights. The table next to that displays the top 10 style matchups by the percentage of red wins, along with the number of fights in which that specific match-up has occurred. Finally, graphs are plotted to display the distributions of activity scores, finishing scores, and factor scores. One important part to note is the extreme variation that is seen in both rd factor 2 and blue factor two, which is not otherwise seen.

```{r, echo=FALSE}
# This section performs exploratory data analysis (EDA) on key features of the dataset.
# It includes summary statistics, distributions, and visualizations to understand data characteristics.
# -----------------------------------------------------------------------------------

# Summary Statistics
summary_stats <- cleaned_data %>%
  summarise(
    avg_age_r = mean(r_age, na.rm = TRUE),
    avg_age_b = mean(b_age, na.rm = TRUE),
    avg_odds_r = mean(r_odds, na.rm = TRUE),
    avg_odds_b = mean(b_odds, na.rm = TRUE),
    avg_activity_r = mean(r_activity_score, na.rm = TRUE),
    avg_activity_b = mean(b_activity_score, na.rm = TRUE),
    avg_finishing_r = mean(r_finishing_score, na.rm = TRUE),
    avg_finishing_b = mean(b_finishing_score, na.rm = TRUE)
  )


(summary_stats)%>%
kable(escape = TRUE,digits = 2, caption = "Summary Statistics") %>%  kable_styling(full_width = FALSE, latex_options = "scale_down")

# Combine Red and Blue Age Distribution
ggplot(cleaned_data) +
  geom_histogram(aes(x = r_age, fill = "Red"), binwidth = 2, alpha = 0.5, color = "black") +
  geom_histogram(aes(x = b_age, fill = "Blue"), binwidth = 2, alpha = 0.5, color = "black") +
  scale_fill_manual(values = c("Red" = "red", "Blue" = "blue")) +
  labs(
    title = "Age Distribution of Fighters",
    x = "Age",
    y = "Count",
    fill = "Fighter Corner"
  ) +
  theme_minimal()

# Combine Odds Distribution
ggplot(cleaned_data) +
  geom_histogram(aes(x = r_odds, fill = "Red"), binwidth = 0.05, alpha = 0.5, color = "black") +
  geom_histogram(aes(x = b_odds, fill = "Blue"), binwidth = 0.05, alpha = 0.5, color = "black") +
  scale_fill_manual(values = c("Red" = "red", "Blue" = "blue")) +
  labs(
    title = "Odds Distribution for Fighters",
    x = "Odds",
    y = "Count",
    fill = "Fighter Corner"
  ) +
  theme_minimal()

# Winning Rate by Weight Class
win_rate_by_weight_class <- cleaned_data %>%
  group_by(weight_class) %>%
  summarise(
    red_win_rate = mean(winner, na.rm = TRUE),
    total_fights = n()
  )


(win_rate_by_weight_class)%>%
kable(escape = TRUE,digits = 2, caption = "Winning Rate by Weight Class") %>%
kable_styling(full_width = FALSE)

ggplot(win_rate_by_weight_class, aes(x = weight_class, y = red_win_rate)) +
  geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
  labs(
    title = "Winning Rate of Red Fighters by Weight Class",
    x = "Weight Class",
    y = "Winning Rate"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Style Matchup Impact
style_matchup_summary <- cleaned_data %>%
  group_by(style_matchup) %>%
  summarise(
    avg_win_rate = mean(winner, na.rm = TRUE),
    total_fights = n()
  ) %>%
  arrange(desc(avg_win_rate))


style_matchup_summary %>% head(10) %>%
kable(escape = TRUE,digits = 2, caption = "Top 10 Style Matchup Win Rates") %>% kable_styling(full_width = FALSE)

# Combine Activity Score Distribution
ggplot(cleaned_data) +
  geom_histogram(aes(x = r_activity_score, fill = "Red"), binwidth = 0.5, alpha = 0.5, color = "black") +
  geom_histogram(aes(x = b_activity_score, fill = "Blue"), binwidth = 0.5, alpha = 0.5, color = "black") +
  scale_fill_manual(values = c("Red" = "red", "Blue" = "blue")) +
  labs(
    title = "Activity Score Distribution for Fighters",
    x = "Activity Score",
    y = "Count",
    fill = "Fighter Corner"
  ) +
  theme_minimal()

# Combine Finishing Score Distribution
ggplot(cleaned_data) +
  geom_histogram(aes(x = r_finishing_score, fill = "Red"), binwidth = 0.5, alpha = 0.5, color = "black") +
  geom_histogram(aes(x = b_finishing_score, fill = "Blue"), binwidth = 0.5, alpha = 0.5, color = "black") +
  scale_fill_manual(values = c("Red" = "red", "Blue" = "blue")) +
  labs(
    title = "Finishing Score Distribution for Fighters",
    x = "Finishing Score",
    y = "Count",
    fill = "Fighter Corner"
  ) +
  theme_minimal()






# Select and reshape factor data for visualization
factor_data <- cleaned_data %>%
  dplyr::select(starts_with("r_factor"), starts_with("b_factor")) %>%
  pivot_longer(cols = everything(), names_to = "factor", values_to = "value")

# Example visualization of factor data
ggplot(factor_data, aes(x = factor, y = value, fill = factor)) +
  geom_boxplot() +
  labs(
    title = "Distribution of Factors for Fighters",
    x = "Factors",
    y = "Values"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



```

  Age is an important predictor of fight outcomes, and so it is also important to analyze how it impacts winning probability. In the following section, Win probabilities for each age are plotted for all ages with at least 30 fighters fighting at that age, to ensure reasonable sample sizes. From the plot, it is clear that there is a sharp downward turn. However, the relationship might be more complicated than what is visually obvious. Fighters tend to face weak competition when they are younger and first entering the UFC, often hitting their competitive primes around the age of 30, before slowing down and diminishing in performance ability. As such, the "fighter ability over time" graph is generally thought of as being quadratic rather than linear.


```{r, echo=FALSE}
# This code section calculates the winning rate by age for fighters, considering both Red and Blue fighters,
# and visualizes the relationship between age and winning probability using a line plot.
# -----------------------------------------------------------------------------------

# Step 1: Prepare data for Red and Blue fighters
red_fighter_data <- cleaned_data %>%
  dplyr::select(age = r_age, win = winner)  # 'winner' is 1 for Red win

blue_fighter_data <- cleaned_data %>%
  dplyr::select(age = b_age) %>%
  mutate(win = ifelse(cleaned_data$winner == 0, 1, 0))  # 1 for Blue win (inverse of 'winner')

# Step 2: Combine Red and Blue fighter data into a single dataset
age_win_data <- bind_rows(red_fighter_data, blue_fighter_data) %>%
  group_by(age) %>%
  summarise(
    total_fighters = n(),  # Count total fighters at each age
    winning_rate = mean(win, na.rm = TRUE)  # Calculate winning rate
  ) %>%
  filter(total_fighters >= 30)  # Keep only ages with at least 30 fighters

# Step 3: Plot the winning rate by age
ggplot(age_win_data, aes(x = age, y = winning_rate)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  labs(
    title = "Winning Rate of Fighters by Age",
    x = "Age",
    y = "Winning Rate"
  ) +
  theme_minimal()


```

  These exploratory insights suggest that while some patterns, like fighter activity, may already be incorporated and partially incorporated betting odds, other nuanced factors, such as style match ups and variability in specific factors, may present opportunities for better prediction models.


\newpage

# Methodology

## Statistical Models

  In order to make predictions on the binary fight outcomes and better understand the relationship between predictors and the outcome, we will now proceed to test several statistical methods aimed at properly classifying fight outcomes. This will come in two primary forms, first is a series of logistic regression models, aimed at predicting the outcome of a fight based on its regressors. Logistic regression offers a straightforward way of estimating the probability of a chance event, making it particularly suited for binary predictions such as fight winners. The second main form we will use is the discriminant analysis model, which will let us form decision boundaries which will allow us to classify fights into outcomes based on predictor values using discriminant functions. Discriminant analysis, on the other hand, performs well at classification when predictors follow multivariate normal distributions, creating sharp decision boundaries for classification. For both forms of analysis, we will employ cross-validation to maintain robustness and guard against overfitting. For computational efficiency, k-fold cross-validation is run with a default k of 5.


### Logistic Regresion Models

  First, we begin with the logistic regression models. As mentioned, f-fold cross-validation is used to ensure that model performance is evaluated on multiple subsets of the data, reducing the chance of fitting and providing a more robust estimation. The following section of code forms the cross-validated GLM logistic regression models for several of the key variables. The r_odds model employs only r_odds as a predictor of the winner. All other models also include r_odds as a predictor. The 35_curse model tests the hypothesis that fighters aged 35 or older are at a disadvantage, while the factors model evaluates the predictive power of the latent variables derived from factor analysis. The style_matchup and style models employ the style_match up and style variables respectively, which were gathered from the k-means clustering analysis.

  The regression tables of these models are then displayed one by one. Notably, in all models, r_odds is an extremely significant predictor. Notably, none of the coefficients are statistically significant in the style matchup regression other than the r_odds variable, which can be interpreted as none of the style matchups leading to statistically significant differences from the "Balanced vs. Balanced" baseline matchup. For the style table, the only statistically significant coefficient is PowerStriker, which performs worse than the baseline of BalancedWrestler. For the activity score regression, BOTh the red and blue activity scores are statistically significant under an alpha of 0.05 even when taking the odds into account. Additionally, the coefficients are what we would expect, red fighters being more active and blue fighters being less active both contribute to a higher chance of a red victory. For the finishing score variable, once again the coefficient for the blue finishing score is statistically significant and negative, suggesting bettors are overbetting on fighters with a high finishing rate. Next, for the 35 or older variable, neither coefficient is statistically significant. As age is an objective measurement and widely available, this is intuitive. Finally, the factors model is quite strange in that the only significant coefficients aside from the significant odds are the third factors for red and blue fighters. This is strange both because these factors represent differing qualities, and also because these variables exhibited much less variation across fighters than the second factor, neither of which is significant. For now, we have constructed and analyzed several basic linear models. After we will  move on to create two models which are slightly more complicated in terms of their construction.

```{r, echo=FALSE, results = 'asis'}
# This code section summarizes logistic regression models into a single table.
# -----------------------------------------------------------------------------------
final_data <- cleaned_data

# Ensure the outcome variable 'winner' is a factor for binary classification
final_data$winner <- as.factor(final_data$winner)

# Set up cross-validation
cv_folds <- trainControl(method = "cv", number = 5, savePredictions = "final")

# Run and summarize selected logistic regression models
# LM with just r_odds as the predictor
lm_r_odds <- train(
  winner ~ r_odds, 
  data = final_data, 
  method = "glm", 
  family = "binomial", 
  trControl = cv_folds
)



# LM with r_odds and style_matchup as predictors
lm_style_matchup <- train(
  winner ~ r_odds + style_matchup, 
  data = final_data, 
  method = "glm", 
  family = "binomial", 
  trControl = cv_folds
)

# LM with r_odds and styles predictors
lm_style <- train(
  winner ~ r_odds + r_style_cluster + b_style_cluster, 
  data = final_data, 
  method = "glm", 
  family = "binomial", 
  trControl = cv_folds
)

# LM with r_odds, blue and red activity scores as predictors
lm_activity <- train(
  winner ~ r_odds + b_activity_score + r_activity_score, 
  data = final_data, 
  method = "glm", 
  family = "binomial", 
  trControl = cv_folds
)

# LM with r_odds, blue and red finishing scores as predictors
lm_finishing <- train(
  winner ~ r_odds + b_finishing_score + r_finishing_score, 
  data = final_data, 
  method = "glm", 
  family = "binomial", 
  trControl = cv_folds
)

# LM with r_odds and age over 34 for red and blue fighters
lm_35_curse <- train(
  winner ~ r_odds + r_age_over_34 + b_age_over_34, 
  data = final_data, 
  method = "glm", 
  family = "binomial", 
  trControl = cv_folds
)



lm_factors <- train(
  winner ~ r_odds + r_age_over_34 + b_age_over_34 + 
    r_factor1_StrikingAbility + r_factor2_ExperienceAndResilience + 
    r_factor3_GrapplingAbility + r_factor4_OpponentGrapplingCaliber + 
    b_factor1_StrikingAbility + b_factor2_ExperienceAndResilience + 
    b_factor3_GrapplingAbility + b_factor4_OpponentGrapplingCaliber, 
  data = final_data, 
  method = "glm", 
  family = "binomial", 
  trControl = cv_folds
)


glm_r_odds <- lm_r_odds$finalModel
glm_style_matchup <- lm_style_matchup$finalModel
glm_style <- lm_style$finalModel
glm_activity <- lm_activity$finalModel
glm_finishing <- lm_finishing$finalModel
glm_35_curse <- lm_35_curse$finalModel
glm_r_odds <- lm_r_odds$finalModel
glm_factors <- lm_factors$finalModel


options(scipen = 999)

# Extract model coefficients and convert them to a tidy format
model_list <- list(
  r_odds = tidy(glm_r_odds),
  style_matchup = tidy(glm_style_matchup),
  style=tidy(glm_style),
  activity = tidy(glm_activity),
  finishing = tidy(glm_finishing),
  curse = tidy(glm_35_curse),
  factors = tidy(glm_factors)
)


# Print tidy summaries for each model
for (model_name in names(model_list)) {
  # cat("\nModel:", model_name, "\n")
  
  # Extract the tidy data frame for the model
  model_summary <- model_list[[model_name]]
  
caption_text <- paste0("Logistic Regression Summary: ", gsub("_", "-", model_name))

model_pretty <- model_summary %>%
  kable(escape = TRUE, digits = 2, caption = caption_text) %>%
  kable_styling(full_width = FALSE, latex_options = "scale_down")

print(model_pretty)
  
}




# View(summary_table)

```


  Next, we will use stepwise regression in order to generate a model, the stepwise model, that selects the best predictive variables based on AIC. Stepwise regression can be used to systematically select predictive variables, one at a time, picking the variable that contributes most to the model's accuracy while. By starting from a null model iteratively adding or removing predictors, and judging using AIC rather than direct fit, stepwise regression can ensure that only the variables with the most significant impact on the outcome remain in the final model. One notable limitation to this stepwise regression is that it can miss interactions or nonlinear effects, and still may deal with overfitting. The stepwise regression is also applied across the full range of available predictors, including novel variables derived from previous sections of the paper.

  To ensure that the stepwise regression algorithm functions efficiently and accurately, we first begin by preprocessing the data and removing variables with excessive missing values or high cardinality (categorical variables with many classes). This is done to make sure the code can run, as well as the fact that those with numerous unique categories can run to issues involving computational inefficiency and overfitting. By evaluating the model's performance using multiple data splits through 5-fold cross-validation, we can assess how well the stepwise regression does at generalizing to unseen data. As there are approximately 200 variables, and all results are cross-validated, this section of code may take some time to run. The code begins by filtering out problematic columns which would impede the stepwise regression, such as those with many NA values, or large categorical variables. Finally, stepwise regression is performed, and the final model is displayed in the output below.

  The final stepwise model's output includes the coefficients, standard errors, and p-values of the selected predictors. These results help identify which variables are most impactful in predicting fight outcomes. Interestingly, the only novel variable that is selected in the final stepwise model is the binary Vegas variable. This variable's lack of complexity likely explains why it, over a variable like style_matchup with 35 classes, was chosen.

```{r, echo=FALSE}
# This code section performs preprocessing to remove problematic columns, reduce dimensionality, 
# and execute stepwise regression to select the best predictive variables based on AIC.
# -----------------------------------------------------------------------------------

# Identify columns with 50 or fewer NA values
columns_to_keep <- colSums(is.na(final_data)) <= 10


cut_columns <- colSums(is.na(final_data)) > 10
cut_columns <- final_data[cut_columns]


# Filter out columns with more than 50 NA values
final_data <- final_data[, columns_to_keep]

# Verify the updated dataset
missing_summary <- colSums(is.na(final_data))
missing_summary <- missing_summary[missing_summary > 0]

stepwise_data <- final_data %>%
  dplyr::select(where(~ n_distinct(.) > 1))

# Remove columns with more than 10 unique categories/strings
stepwise_data <- stepwise_data %>%
  dplyr::select(where(~ is.numeric(.) || n_distinct(.) <= 10))



# Filter complete cases for the stepwise regression
stepwise_data <- stepwise_data %>%
  dplyr::filter(complete.cases(.))

# Define full and null models
full_model <- glm(
  winner ~ ., 
  data = stepwise_data, 
  family = "binomial"
)
null_model <- glm(
  winner ~ 1, 
  data = stepwise_data, 
  family = "binomial"
)

# Perform stepwise regression
lm_stepwise <- stepAIC(
  object = null_model, 
  scope = list(lower = null_model, upper = full_model), 
  direction = "both", 
  trace = FALSE
)

# Summary of the final model
lm_stepwise%>% tidy(conf.int=TRUE) %>% head(10) %>%
kable(escape = TRUE,digits = 2, caption = "Stepwise Logistic Regression Model Summary (First 10 rows)") %>% kable_styling(full_width = FALSE)



```

  The next model, noted as the "super" model is aimed at integrating the strengths of both the stepwise-selected variables and the novel variables we developed in our previous analysis. By combining these sets, we hope to capture a more comprehensive range of predictive factors that influence fight outcomes. While this does sacrifice some complexity and statistical power, it does so to lead to a higher prediction ability. While stepwise regression is mostly effective in selecting variables that optimize AIC values, it may exclude important predictors that have practical significance due to complexity or multicollinearity. An example of a variable like this is style_matchup, which may be excluded despite its potential predictive power. Additionally, looking at the prediction accuracy of both the stepwise model and the super model, we can identify the specific growth in accuracy caused by the novel variables, as only the Vegas variable is included in the stepwise model. The manually created variables are potentially useful in measuring nuanced aspects of fighter performance and strategy that are not reflected in the standard fight statistics. As much of this analysis is aimed at finding new information, we feel it is best to also run a model with the novel variables as it maximizes that chance.

```{r, echo=FALSE}
# This code section combines the stepwise-selected variables with the in-code-created variables to fit a final "super" model
# -----------------------------------------------------------------------------------

# Step 1: Define the manually created variables to include in the final model
manual_vars <- c(
  "style_matchup",
  "b_activity_score",
  "r_activity_score",
  "b_finishing_score",
  "r_finishing_score",
  "r_prior_fights",
  "b_prior_fights",
  "vegas",
  "r_age_over_34",
  "b_age_over_34",
  "r_factor1_StrikingAbility",
  "r_factor2_ExperienceAndResilience",
  "r_factor3_GrapplingAbility",
  "r_factor4_OpponentGrapplingCaliber",
  "b_factor1_StrikingAbility",
  "b_factor2_ExperienceAndResilience",
  "b_factor3_GrapplingAbility",
  "b_factor4_OpponentGrapplingCaliber"
)



# Step 2: Extract the variables from the stepwise-selected model
stepwise_vars <- all.vars(lm_stepwise$call$formula)

# Step 3: Combine stepwise-selected variables with the manually created variables
final_vars <- unique(c(stepwise_vars, manual_vars))

final_vars <- final_vars[!final_vars=="r_win_by_decision_split"]

# Step 4: Create a formula for the final model
final_model_formula <- as.formula(
  paste("winner ~", paste(final_vars, collapse = " + "))
)

# Step 5: Fit the final model with both stepwise-selected and manually created variables
lm_super <- glm(
  formula = final_model_formula, 
  data = final_data, 
  family = "binomial"
)

# Step 6: Summarize the final model
(lm_super)%>% tidy(conf.int=T) %>% head(10) %>%
kable(escape = TRUE,digits = 2, caption = "Logistic Regression Super Model Summary (First 10 rows)") %>%  kable_styling(full_width = FALSE, latex_options = "scale_down")

```

  Finally, the below code focuses on implementing LASSO regression, which was chosen as it is capable of performing variable selection by penalizing the absolute size of coefficients, effectively setting the less useful predictors to zero. hHis provides an alternative way of addressing multicollinearity and helping to prevent over fitting. Given the extremely high dimensionality of the dataset, having well over 200 variables, LASSO's ability to streamline predictors is potnetially helpful in improving model interpretability and computational efficiency. Cross-validation is once again implemented as it ensures that the optimal lambda value is robust and not overfitting the training data. A cross-validated LASSO regression was run, and the accuracy of this model is noted as being 0.6161. Notably, this was lower than that of other logistic regression models, even those which just include r_odds and nothing else, as will be seen later in section five. This suggests that although LASSO is effective at variable selection, it may not fully capture the complexity of fight outcomes when it comes to predicting fight outcomes. Finally, the included variables are listed in descending order of the coefficient in the model. The inclusion of red_factor4 and both finishing score variables underscores the potential impact of ground control and finishing ability in determining fight outcomes, suggesting that these aspects of fighting may not be fully captured by the current betting odds.

```{r, echo=FALSE}
# LASSO Logistic Regression for Fight Outcome Prediction
# -----------------------------------------------------------------------------------

# Load required libraries
library(glmnet)

# Step 1: Prepare Dataset
X <- final_data[, -which(names(final_data) == "winner")]
X <- X[, sapply(X, is.numeric)]  # Keep numeric columns only
X <- X[, !grepl("odds|predict|expected", names(X))]  # Exclude specific columns
Y <- as.numeric(final_data$winner) - 1  # Convert 'winner' to binary (1 or 0)

# Step 2: Handle One-Hot Encoding for 'style_matchup'
if ("style_matchup" %in% names(final_data)) {
  X_encoded <- model.matrix(~ style_matchup - 1, data = final_data)  # One-hot encode
  X <- cbind(X, X_encoded)  # Combine with existing data
}

# Step 3: Split Data into Training and Testing Sets
set.seed(42)  # For reproducibility
train_index <- sample(1:nrow(X), size = 0.8 * nrow(X), replace = FALSE)
trainX <- as.matrix(X[train_index, ])
trainY <- Y[train_index]
testX <- as.matrix(X[-train_index, ])
testY <- Y[-train_index]

# Step 4: Perform LASSO Logistic Regression with Cross-Validation
cv_lasso <- cv.glmnet(trainX, trainY, alpha = 1, family = "binomial")


# Step 5: Extract Optimal Lambda
best_lambda <- cv_lasso$lambda.min
cat("Optimal Lambda:", best_lambda, "\n")

# Step 6: Make Predictions on Test Set
predicted_probs <- predict(cv_lasso, newx = testX, s = best_lambda, type = "response")
predictions <- ifelse(predicted_probs > 0.5, 1, 0)

# Step 7: Evaluate Model Accuracy
accuracy <- mean(predictions == testY)
cat("Accuracy:", accuracy, "\n")

# Step 8: Extract Non-Zero Coefficients
feature_importance <- coef(cv_lasso, s = best_lambda)
feature_importance <- as.data.frame(as.matrix(feature_importance))
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance)[1] <- "Coefficient"
feature_importance <- feature_importance[feature_importance$Coefficient != 0, ]

# Step 9: Display Important Features
feature_importance <- feature_importance[order(-abs(feature_importance$Coefficient)), ]

(feature_importance)%>% head(10) %>%
kable(escape = TRUE,digits = 2, caption = "Top 10 Significant Features by Coefficent Size") %>%  kable_styling(full_width = FALSE, latex_options = "scale_down")

```

### Discriminant Analysis Models

  The next section of the paper focuses on using discriminant analysis to differentiate between fights resulting in red wins and losses. We begin with linear discriminant analysis (LDA), the simplest form of discriminant analysis. To evaluate the suitability of LDA for this dataset, we examined two key assumptions: the multivariate normality of predictor within each class, and the equality of covariance matrices across classes. The Henze-Zirkler multivariate normality test revealed significant deviations from multivariate normality for the predictors (p < 0.05) within both classes. Additionally, Box's M test indicated substantial differences in covariance matrices across classes (p < 0.05). These results suggest that LDA may not be ideal for this dataset, as violations of these assumptions can affect its reliability. Nonetheless, we proceed with LDA as a baseline model, acknowledging its limitations and considering alternative approaches like quadratic discriminant analysis (QDA).

  To implement LDA, we then used a subset of predictors after handling missing data and transforming variables to improve normality.The model was trained on these predictors to create a linear decision boundary, which was then used to classify fights into predicted outcomes. Using 5-fold cross-validation, the overall misclassification rate was found to be 0.3611, corresponding to a prediction accuracy of approximately 64%. As discussed in Section 6, this accuracy is comparable to the logistic regression models created in the previous section, reinforcing LDA's value as a baseline approach despite its theoretical limitations for this dataset.

```{r, echo=FALSE, results = 'asis'}
# Tests for assumptions of LDA, then runs LDA on the subsetted data.
#----------------------------------------------------------------------
# Step 1: Select numeric predictors and target variable
X_clean <- final_data[, sapply(final_data, is.numeric)]  # Select numeric predictors
y_clean <- as.factor(final_data$winner)  # Convert target to factor

# Step 2: Handle Missing Data
# Replace missing values with the column mean
X_clean <- as.data.frame(lapply(X_clean, function(x) {
  if (is.numeric(x)) {
    x[is.na(x) | is.nan(x) | is.infinite(x)] <- mean(x, na.rm = TRUE)  # Handle NA, NaN, Inf
  }
  return(x)
}))

# Step 3: Transform Variables to Improve Normality
transform_vars <- function(x) {
  if (all(x > 0, na.rm = TRUE)) {
    return(log(x + 1))  # Apply log transformation if all values are positive
  } else {
    return(sqrt(x + abs(min(x, na.rm = TRUE)) + 1))  # Apply square root if negatives exist
  }
}
X_clean_transformed <- as.data.frame(lapply(X_clean, transform_vars))

# Remove duplicates and constant columns
X_clean_transformed <- X_clean_transformed[, sapply(X_clean_transformed, is.numeric)]
X_clean_transformed <- X_clean_transformed[!duplicated(X_clean_transformed), ]
constant_cols <- sapply(X_clean_transformed, function(x) var(x, na.rm = TRUE) == 0)
X_clean_transformed <- X_clean_transformed[, !constant_cols]

# Step 4: Multivariate Normality Testing

# Dimensionality reduction for high-dimensional data
if (ncol(X_clean_transformed) > nrow(X_clean_transformed)) {
  pca_result <- prcomp(X_clean_transformed, scale. = TRUE)
  X_clean_transformed <- as.data.frame(pca_result$x[, 1:10])  # Retain first 10 components
}

# Test for multivariate normality by class
if (nrow(X_clean_transformed) > 1 && ncol(X_clean_transformed) > 1) {
  classes <- unique(y_clean)
  cat("Testing multivariate normality for each class separately:\n")
  
  for (class in classes) {
    class_data <- X_clean_transformed[y_clean == class, ]
    cat("\nClass:", class, "\n")
    
    # Check if the class has sufficient rows for testing
    if (nrow(class_data) > 1) {
      mult_test_result <- mvn(data = class_data, mvnTest = "hz")
      cat("Henze-Zirkler Test Results:\n")
      print((mult_test_result$multivariateNormality)%>%
kable(escape = TRUE,digits = 2, caption = paste0("Henze-Zirkler Test Results: Class ",class)) %>% kable_styling(full_width = FALSE))
    } else {
      cat("Insufficient data for multivariate normality testing in class:", class, "\n")
    }
  }
} else {
  cat("Insufficient data for overall multivariate normality testing.\n")
}



# Test for covariance equality 
if (nrow(X_clean_transformed) > 1 && ncol(X_clean_transformed) > 1) {
  boxm_result <- boxM(X_clean_transformed, grouping = y_clean)
  cat("\nBox's M Test Results:\n")
  print(boxm_result)
  
  # Interpretation of Results
  if (boxm_result$p.value < 0.05) {
    cat("\nWarning: The assumption of equal covariance matrices is violated (p < 0.05).\n")
  } else {
    cat("\nThe assumption of equal covariance matrices holds (p >= 0.05).\n")
  }
} else {
  cat("Insufficient data for testing equality of covariance matrices.\n")
}

# Step 5: Perform LDA
if (nrow(X_clean_transformed) > 1 && ncol(X_clean_transformed) > 1) {
  lda_model <- lda(y_clean ~ ., data = X_clean_transformed)
  # cat("\nLDA Model Summary:\n")
  # print(lda_model)
} else {
  cat("Insufficient data for LDA.\n")
}

# Step 6: Cross-Validation and Evaluation
folds <- sample(1:5, size = nrow(X_clean_transformed), replace = TRUE)
misclassification_rates <- numeric(5)

for (i in 1:5) {
  # Split into training and test sets
  train_indices <- which(folds != i)
  test_indices <- which(folds == i)
  
  X_train <- X_clean_transformed[train_indices, ]
  y_train <- y_clean[train_indices]
  X_test <- X_clean_transformed[test_indices, ]
  y_test <- y_clean[test_indices]
  
  # Train LDA on training set
  lda_cv_model <- lda(y_train ~ ., data = data.frame(X_train, y_train = y_train))
  
  # Make predictions on test set
  predictions <- predict(lda_cv_model, newdata = data.frame(X_test))$class
  
  # Calculate misclassification rate for this fold
  misclassification_rates[i] <- mean(predictions != y_test)
}

# Calculate and print the overall misclassification rate
if (length(misclassification_rates) > 0) {
  cv_misclassification_rate <- mean(misclassification_rates)
  cat("\n5-Fold Cross-Validated Misclassification Rate:", cv_misclassification_rate, "\n")
} else {
  cat("Cross-validation could not be performed.\n")
}

```

  Next, we instead fit a Quadratic Discriminant Analysis model on the data, making sure to include cross-validation.QDA differs from LDA in terms of flexibility as it allows for different covariance matrices for each class. Keep in mind that this data, although no longer needing to have equal covariance matrices, still fails to meet the in-class multivariate normality assumption. Due to the high multicollinearity of the data, some transformations and dimension-reduction techniques must be applied to run the analysis. The below model specifically works by computing PCA on the initial numeric predictors and then using the principal components of that PCA model as predictors in the discriminant analysis model. PCA is useful in addressing multicollinearity as the principal components will always orthogonal/independent, and can be selected to capture a specific percentage of the variation within the data. However, PCA can also potentially lead to information loss if the retained components do not capture all relevant variance for group separation. By then using discriminant analysis on the principal components, we can determine new multivariate directions in space that discriminate between winning and losing fights, allowing us to better understand how variation in predictors relates to differences in outcomes. It must also be noted that the discriminant functions will not be as easily interpretable as if they had used the variables directly rather than the principal components. Cross-validation meanwhile ensures the results are robust and generalizable. The following code runs QDA, using a for-loop to determine the optimal number of components to include from the PCA in the QDA. The optimal number of components is determined to be 15, which leads to a misclassification rate of 0.38548, leading to an accuracy rate of approximately 61%. This is marginally worse than the LDA model and the logistic regression models, suggesting that the data violates the assumptions too much to be accurate or produce gains in predictive power. 

```{r, echo=FALSE}
# Quadratic Discriminant Analysis (QDA) with  PCA and  5-Fold CV: Model Building and Evaluation
# -----------------------------------------------------------------------------------

# Initialize variables to track the optimal number of components and misclassification rate
min_misclassification_rate <- Inf
optimal_num_components <- 0

# Define the maximum number of components to test
max_components_to_test <- min(ncol(X_clean), 50)  # Adjust for efficiency
misclassification_rates <- numeric(max_components_to_test)  # Preallocate vector

# Loop through different numbers of principal components
for (num_components in 1:max_components_to_test) {
  set.seed(42)  # For reproducibility
  
  # Perform 5-fold cross-validation
  folds <- createFolds(y_clean, k = 5, list = TRUE)  # Use 5 folds
  misclassifications <- 0
  
  for (fold_indices in folds) { 
    # Partition the data into training and testing sets
    X_train <- X_clean[-fold_indices, ]
    y_train <- y_clean[-fold_indices]
    X_test <- X_clean[fold_indices, ]
    y_test <- y_clean[fold_indices]
     zero_variance_columns <- apply(X_train, 2, function(col) var(col) == 0)
    X_train <- X_train[, !zero_variance_columns, drop = FALSE]
    X_test <- X_test[, !zero_variance_columns, drop = FALSE]
    
    
    # Perform PCA on the training set only
    pca <- prcomp(X_train, center = TRUE, scale. = TRUE)
    
    # Extract the first `num_components` principal components for training and testing
    X_train_pca <- as.data.frame(pca$x[, 1:num_components])
    X_test_pca <- as.data.frame(predict(pca, newdata = X_test)[, 1:num_components])
    
    # Rename principal component columns to avoid conflicts
    colnames(X_train_pca) <- paste0("PC", 1:num_components)
    colnames(X_test_pca) <- paste0("PC", 1:num_components)
    
    # Combine PCA-transformed data with the target variable
    train_set <- cbind(X_train_pca, y_clean = y_train)
    test_set <- cbind(X_test_pca, y_clean = y_test)
    
    # Train QDA model on the training set
    qda_model <- qda(y_clean ~ ., data = train_set)
    
    # Predict on the test set
    qda_prediction <- predict(qda_model, newdata = test_set)$class
    
    # Count misclassifications
    misclassifications <- misclassifications + sum(qda_prediction != test_set$y_clean)
  }
  
  # Calculate the misclassification rate
  misclassification_rate <- misclassifications / length(y_clean)
  
  # Store the misclassification rate
  misclassification_rates[num_components] <- misclassification_rate
  
  # Update the optimal number of components if this rate is lower
  if (misclassification_rate < min_misclassification_rate) {
    min_misclassification_rate <- misclassification_rate
    optimal_num_components <- num_components
  }
}

# Output the optimal number of components and the corresponding misclassification rate
results_df <- data.frame(
  Metric = c("Optimal Number of Components", "Minimum Misclassification Rate"),
  Value = c(optimal_num_components, min_misclassification_rate),
  stringsAsFactors = FALSE
)
results_df %>%
  kable(escape = TRUE,digits = 3, caption = "PCA Tuning Results") %>%
  kable_styling(full_width = FALSE)

# Plot misclassification rates against the number of components
plot(1:max_components_to_test, misclassification_rates, type = "b",
     xlab = "Number of Principal Components", ylab = "Misclassification Rate",
     main = "Misclassification Rate vs. Number of Components")

```

  Our final QDA model looks at implementing QDA without first filtering the variables through PCA. This would present issues if done directly as previously mentioned there is an extreme amount of multicollinearity among the predictor variables. As such, the below code checks for and removes linear dependencies by removing variables with high variance inflation factor scores, until all of the variables have VIF scores of less than 10, which is a standard rule of thumb. This ensures that variables that disproportionately contribute to multicollinearity are removed. This results in a final model of approximately 60 variables. Note that when running this code section, it takes a very long time to run due to the high number of starting variables, and the fact that VIF is context-dependent, which means it needs to be recalculated after each removed variable. It should also be noted that the r_odds variable, due to its importance in predictive power, is excluded from being removed to ensure it remains in the final model. With a classification rate of 0.4450, this model performs very poorly and does worse than a model that only including r_odds to make the prediction. This is somewhat surprising, as we would expect a QDA model based on the baseline variables to be mostly comparable to LDA results, or the combined PCA QDA results, but are starkly worse. QDA might be more sensitive to violations of assumptions, as well as the idea that the chosen predictors, even after cleaning, may lack strong relationships with the target. A potential alternative model which could be used in future research could be using regularized discriminant analysis.


```{r, echo=FALSE}
# Quadratic Discriminant Analysis (QDA) without PCA: Model Building and Evaluation
# This code section implements QDA to predict fight outcomes (winning vs. losing) 
# without dimensionality reduction techniques like PCA.
# -----------------------------------------------------------------------------------

# Step 1: Move 'r_odds' to the end of the dataset
final_data_DA <- final_data[, sapply(final_data, is.numeric)]
final_data_DA <- final_data_DA[, c(setdiff(names(final_data_DA), "r_odds"), "r_odds")]

# Step 2: Remove constant variables
final_data_DA <- final_data_DA[, sapply(final_data_DA, function(col) {
  length(unique(col)) > 1
})]

# Step 3: Check for linear dependencies (multicollinearity)

# Iteratively remove variables with VIF > 10, but skip 'r_odds'
while (TRUE) {
  vif_values <- calc_vif(final_data_DA[, -ncol(final_data_DA)])  # Exclude 'r_odds' for VIF calculation
  max_vif <- max(vif_values, na.rm = TRUE)
  if (max_vif > 10) {
    # Remove the variable with the highest VIF
    var_to_remove <- names(which.max(vif_values))
    # cat("Removing variable due to high VIF:", var_to_remove, "\n")
    final_data_DA <- final_data_DA[, !names(final_data_DA) %in% var_to_remove]
  } else {
    break
  }
}

# Step 4: Ensure predictors  observations
num_observations <- nrow(final_data_DA)
num_predictors <- ncol(final_data_DA)

if (num_predictors >= num_observations) {
  # Remove less important variables (e.g., based on correlation with the target)
  cor_with_target <- sapply(final_data_DA, function(col) cor(col, final_data$winner, use = "complete.obs"))
  sorted_vars <- names(sort(abs(cor_with_target), decreasing = TRUE))
  final_data_DA <- final_data_DA[, sorted_vars[1:(num_observations - 1)]]
}

# Add the target variable (assuming 'winner' is the target in final_data)
final_data_DA$winner <- final_data$winner

# Output summary of the final dataset
results_df <- data.frame(
  Metric = c("Number of Observations", "Number of Predictors"),
  Value = c(nrow(final_data_DA), ncol(final_data_DA) - 1),
  stringsAsFactors = FALSE
)
results_df %>%
  kable(escape = TRUE,digits = 0, caption = "QDA Dataset Summary (Without PCA)") %>%
  kable_styling(full_width = FALSE)

# Step 5: Remove missing values
final_data_DA <- na.omit(final_data_DA)

nzv <- nearZeroVar(final_data_DA, saveMetrics = TRUE)
final_data_DA <- final_data_DA[, !nzv$nzv]
cor_matrix <- cor(final_data_DA[, -ncol(final_data_DA)], use = "pairwise.complete.obs")  # Exclude 'winner'
high_cor_vars <- findCorrelation(cor_matrix, cutoff = 0.9, names = TRUE)
final_data_DA <- final_data_DA[, !names(final_data_DA) %in% high_cor_vars]


# Step 6: Train QDA Model
qda_model <- qda(winner ~ ., data = final_data_DA)

# Summary of the model
# print(qda_model)

# Step 7: Cross-Validation
set.seed(42)
folds <- createFolds(final_data_DA$winner, k = 5, list = TRUE)

misclassification_rates <- numeric(length(folds))
for (i in seq_along(folds)) {
  test_indices <- folds[[i]]
  train_data <- final_data_DA[-test_indices, ]
  test_data <- final_data_DA[test_indices, ]
  
  # Train QDA model
  qda_model <- qda(winner ~ ., data = train_data)
  
  # Predict on the test set
  predictions <- predict(qda_model, newdata = test_data)
  predicted_classes <- predictions$class
  
  # Calculate misclassification rate
  misclassification_rates[i] <- mean(predicted_classes != test_data$winner)
}

average_misclassification_rate <- mean(misclassification_rates)
cat("Average Misclassification Rate: ", average_misclassification_rate, "\n")


```

## Machine Learning Models

  In the final part of the methods section, we take a look at two models aimed at improving the understanding we have of which variables, constructed or not, are important and helpful in predicting fight outcomes and provide unique information not otherwise captured.

  In the first model, we take a look using a random forest model to assess variable importance. The robustness of random forest models to overfitting ensures that the importance values derived are reliable and have extremely low bias and low variance. Notably, this is true even in datasets with many correlated predictors. In the final plot are two graphs which both emphasize two methods of assessing variable importance. The mean decrease in accuracy indicates how much the model's predictive accuracy declines when a variable is randomly permuted, reflecting its general contribution to prediction accuracy, while a mean decrease in Gini impurity, on the other hand, highlights how much a variable reduces uncertainty in classification across splits. This provides an alternative perspective on importance. In both, r_odds as a predictor is far and away the most important useful predictor, likely due to its direct representation of pre-fight betting expectations, which naturally encapsulate a wealth of contextual information. However, the style matchup, activity score, and finishing score variables all appear to be relatively effective as well, and complement the odds by capturing additional stylistic and performance-based nuances and suggests that they provide unique insights/new information. The differences between the random forest analysis and earlier analyses regarding variables like activity_score may indicate that non-linear relationships play a crucial role in predicting fight outcomes.Finally, there is a reported OOB/CV estimate of  error rate of 37%, which is in line with the logistic regression and discriminant analysis models. 

```{r, echo=FALSE}
# This code section trains a Random Forest model to assess variable importance for predicting UFC fight outcomes.
# -----------------------------------------------------------------------------------

# Ensure the target variable is a factor for classification
final_data$winner <- as.factor(final_data$winner)

# Define the formula for Random Forest
rf_formula <- winner ~ r_odds + r_activity_score + b_activity_score + 
  r_finishing_score + b_finishing_score + style_matchup + age_dif + 
  b_total_rounds_fought + r_total_rounds_fought + b_wins + r_wins + 
  b_height_cms + r_height_cms + b_reach_cms + r_reach_cms + 
  b_current_win_streak + r_current_win_streak + r_current_lose_streak + 
  b_current_lose_streak + b_avg_sig_str_landed + r_avg_sig_str_landed + 
  b_avg_td_pct + r_avg_td_pct + r_wins_by_ko + b_wins_by_ko + 
  r_wins_by_submission + b_wins_by_submission + 
  r_wins_by_tko_doctor_stoppage + b_wins_by_tko_doctor_stoppage 

# Train the Random Forest model
set.seed(123)  # For reproducibility
rf_model <- randomForest(
  formula = rf_formula,
  data = final_data,
  ntree = 500,  # Number of trees
  mtry = 5,     # Number of predictors sampled at each split
  importance = TRUE
)

# Print model summary

rf_model_no_imp <- rf_model
rf_model_no_imp$importance <- NULL
rf_model_no_imp$importanceSD <- NULL

print("Random Forest Model Summary (without importance):")
print(rf_model_no_imp)

# Variable importance analysis
importance_values <- importance(rf_model)  # Extract importance values

importance_values %>% head(10) %>%
  kable(escape = TRUE,digits = 2, caption = "Variable Importance Values (First 10)") %>%
   kable_styling(full_width = FALSE, latex_options = "scale_down")

# Visualize variable importance
varImpPlot(
  rf_model,
  main = "Variable Importance in Random Forest",
  n.var = min(15, nrow(importance_values))  # Show top variables (up to 20)
)

```

  Next, we take a look at a "boosting" model, which attempts to train a boosted tree model using the XGBoost package for binary classification. Boosting algorithms such as XGBoost are highly effective, particularly in capturing non-linear relationships and interactions between variables. This is done by iteratively improving weak learners to create a stronger model while being well-equipped to deal with large datasets and high-dimensional spaces. This machine learning technique allows us an alternative way of measuring variable importance. The feature importance metrics generated by XGBoost are based on how often a feature is used to split data across all boosting rounds, as well as the improvement it provides to the model's performance. Two tables are included at the bottom. The first includes the relative importance of the created variables and the second at all variables. It appears that the factor, activity, and finishing variables are particularly useful, with r_factor3 emerging as the most important created variable. While both Random Forest and XGBoost emphasize the importance of r_odds, it is interesting that XGBoost brings additional attention to predictors such as r_factor3. This potentially indicates a nuanced and specific difference in how the models interpret variable contributions, with the XGBoost model capturing latent patterns captured which are critical in determining fight outcomes. Meanwhile, the inclusion of the style_matchup variable suggests there is potential for trainers and analysts to focus on this as a means of increasing win probabilities. One limitation of XGBoost is its sensitivity to its hyperparameters, such as max_depth and eta, which can impact the stability of feature importance rankings. It is also important to note that these findings are not neccisiarly causal, and may simply be correlated. One area for potential future analysis could be investigating how changing boosting parameters could influence the stability of feature importance ranking.

```{r, echo = FALSE} 
# This code section trains a boosted tree model using XGBoost for binary classification and evaluates feature importance.
# -----------------------------------------------------------------------------------

# Prepare the dataset for training
# Extract features (X) and target (Y)
X <- final_data[, -which(names(final_data) == "winner")]
X <- X[, sapply(X, is.numeric)]  # Keep only numeric columns
X <- X[, !grepl("odds|predict|expected", names(X))]  # Exclude columns with odds, predictions, or expectations

Y <- as.numeric(final_data$winner) - 1  # Convert target to binary (0 for blue win, 1 for red win)

# One-hot encode 'style_matchup'
X$style_matchup <- as.factor(final_data$style_matchup)  # Ensure it's a factor
X_encoded <- model.matrix(~ style_matchup - 1, data = final_data)  # One-hot encoding for 'style_matchup'

# Combine one-hot encoded columns with the main dataset
X <- cbind(X, X_encoded)

# Split the data into training and testing sets
trainIndex <- createDataPartition(Y, p = 0.8, list = FALSE)
trainX <- X[trainIndex, ]
trainY <- Y[trainIndex]
testX <- X[-trainIndex, ]
testY <- Y[-trainIndex]

# Ensure only numeric features for training and testing datasets
trainX_numeric <- trainX[, sapply(trainX, is.numeric)]  # Numeric features for training
testX_numeric <- testX[, sapply(testX, is.numeric)]     # Numeric features for testing

# Convert to DMatrix format for XGBoost
train_data <- xgb.DMatrix(data = as.matrix(trainX_numeric), label = trainY)
test_data <- xgb.DMatrix(data = as.matrix(testX_numeric), label = testY)

# Set parameters for binary classification
params <- list(
  objective = "binary:logistic",  # Binary classification objective
  eval_metric = "error",          # Evaluation metric: classification error
  max_depth = 6,                  # Maximum tree depth
  eta = 0.1,                      # Learning rate
  nthread = 2                     # Number of threads
)

# Train the XGBoost model
boosted_model <- xgb.train(
  params = params,
  data = train_data,
  nrounds = 80,  # Number of boosting rounds
  watchlist = list(train = train_data, test = test_data),
  verbose = 0
)

# Evaluate feature importance
importance_matrix <- xgb.importance(feature_names = colnames(trainX_numeric), model = boosted_model)

# Plot top 15 features using the original importance matrix
xgb.plot.importance(importance_matrix[1:15, ], main = "Top 15 Feature Importance in Boosted Model")

# Now, if you want, create a new column 'Importance' as a copy of 'Gain'
importance_matrix$Importance <- importance_matrix$Gain

# Add ranking based on 'Importance'
importance_matrix$Rank <- rank(-importance_matrix$Importance, ties.method = "first")


# Filter and display important features related to activity, finishing, style, or fights
filtered_importance <- importance_matrix[
  grepl("activity|finishing|style|diff_30|prior_fights|factor", importance_matrix$Feature),
  c("Feature", "Importance", "Rank")
]

# Display filtered importance matrix

(filtered_importance) %>%
  kable(escape = TRUE,digits = 3, caption = "Novel Variables Feature Importance") %>%
  kable_styling(full_width = FALSE)

# Display full importance matrix with ranks
(importance_matrix[, c("Feature", "Importance", "Rank")]) %>% head(10) %>%
  kable(escape = TRUE,digits = 3, caption = "Top 10 Variables by Feature Importance") %>%
  kable_styling(full_width = FALSE)

# Check for missing values in the dataset
missing_values <- list(
  "Column-wise NAs" = colSums(is.na(X)),
  "Row-wise NAs" = rowSums(is.na(X))
)

# Display missing values summary
#print("Missing Values Summary:")
# print(missing_values)

```


\newpage

# Results

  Finally, we will analyze the effectiveness of the logistic regression model changes. Notably, they all perform better at predicting than the discriminant models, and so they will remain the focus of this section. The model's accuracy is tested by assigning a predicted winner for each model according to whether or not the predicted chance of a red victory is greater than 0.5. These predictions are compared to the actual model. As can be seen, the model with the highest accuracy according to this metric is the super_model, notably even outperforming the stepwise model.

## Logistic Model accuracy

  The below code attempts to ascertain the model accuracy of the previously constructed logistic regression models by comparing their predictions against the actual fight outcomes. The evaluation includes both accuracy metrics and a detailed comparison of model choices to the implied odds from betting markets. The "mismatch" analysis, in which models suggest favorites should be different than the betting on predictions, highlights instances where the model diverges significantly from market expectations. These mismatches, particularly when the model exhibits large gaps, may indicate opportunities to identify undervalued bets or to understand discrepancies between statistical and human analyses. The mismatch analysis, however, assumes that the betting market odds are a reasonable baseline for comparison, which may not be true if the odds are heavily influenced by public opinion or other non-statistical factors. A common example of this is fans betting on their favorite fighters even in unfavorable matchups. Although not included in the scope of this analysis, it is also possible to look at large gaps in predicted odds in fights where both models agree on which fighter should be the favorite. 

  The results suggest that the super_model not only improves prediction accuracy but also identifies fights where its predictions diverge meaningfully (Predicting with >60%) from market odds. For example, in Dustin Ortiz v. Brandon Moreno, the official betting odds gave Dustin Ortiz (red) an implied win probability of 52%, whereas this number was 13% for the super model, correctly identifying a Moreno win. All of this could have implications for decision-making in betting, coaching strategies, or fight analysis. Finally, this analysis confirms the importance of the constructed variables, such as style_matchup and activity scores, in improving model predictions. As all models were cross-validated when constructed, these results should be robust. Thus, the novel variables provide domain-specific insights into fight dynamics and highlight the value of integrating expert knowledge, such as the "styles make fights" principle, into model design.

```{r, echo=FALSE}
# This section evaluates logistic regression models by generating predicted odds, 
# identifying mismatches with implied odds, and calculating model accuracies.
# It also analyzes extreme confidence differences in predictions for the "super" model.
# -----------------------------------------------------------------------------------

# Generate predicted odds for each logistic regression model
final_data <- final_data %>%
  mutate(
    predicted_odds_style_matchup = if (inherits(lm_style_matchup, "train")) {
      predict(lm_style_matchup, newdata = final_data, type = "prob")[, 2]
    } else {
      predict(lm_style_matchup, newdata = final_data, type = "response")
    },
    predicted_odds_activity = if (inherits(lm_activity, "train")) {
      predict(lm_activity, newdata = final_data, type = "prob")[, 2]
    } else {
      predict(lm_activity, newdata = final_data, type = "response")
    },
    predicted_odds_finishing = if (inherits(lm_finishing, "train")) {
      predict(lm_finishing, newdata = final_data, type = "prob")[, 2]
    } else {
      predict(lm_finishing, newdata = final_data, type = "response")
    },
    predicted_odds_age = if (inherits(lm_35_curse, "train")) {
      predict(lm_35_curse, newdata = final_data, type = "prob")[, 2]
    } else {
      predict(lm_35_curse, newdata = final_data, type = "response")
    },
    predicted_odds_super = if (inherits(lm_super, "train")) {
      predict(lm_super, newdata = final_data, type = "prob")[, 2]
    } else {
      predict(lm_super, newdata = final_data, type = "response")
    },
    predicted_odds_stepwise = if (inherits(lm_stepwise, "train")) {
      predict(lm_stepwise, newdata = final_data, type = "prob")[, 2]
    } else {
      predict(lm_stepwise, newdata = final_data, type = "response")
    },
    predicted_odds_style = if (inherits(lm_style, "train")) {
      predict(lm_style, newdata = final_data, type = "prob")[, 2]
    } else {
      predict(lm_style, newdata = final_data, type = "response")
    }
  )

final_data <- final_data %>%
  mutate(
    r_odds_pick = ifelse(r_odds >= 0.5, "Red", "Blue"),
    style_matchup_pick = ifelse(predicted_odds_style_matchup > 0.5, "Red", "Blue"),
    activity_pick = ifelse(predicted_odds_activity > 0.5, "Red", "Blue"),
    finishing_pick = ifelse(predicted_odds_finishing > 0.5, "Red", "Blue"),
    age_pick = ifelse(predicted_odds_age > 0.5, "Red", "Blue"),
    super_pick = ifelse(predicted_odds_super > 0.5, "Red", "Blue"),
    stepwise_pick = ifelse(predicted_odds_stepwise > 0.5, "Red", "Blue"),
    style_pick = ifelse(predicted_odds_style > 0.5, "Red", "Blue")
  )

# Identify mismatches for each model
mismatch_results <- list(
  style_matchup = final_data %>%
    filter(r_odds_pick != style_matchup_pick) %>%
    dplyr::select(r_odds, predicted_odds_style_matchup, r_odds_pick, style_matchup_pick),
  activity = final_data %>%
    filter(r_odds_pick != activity_pick) %>%
    dplyr::select(r_odds, predicted_odds_activity, r_odds_pick, activity_pick),
  finishing = final_data %>%
    filter(r_odds_pick != finishing_pick) %>%
    dplyr::select(r_odds, predicted_odds_finishing, r_odds_pick, finishing_pick),
  age = final_data %>%
    filter(r_odds_pick != age_pick) %>%
    dplyr::select(r_odds, predicted_odds_age, r_odds_pick, age_pick),
  super = final_data %>%
    filter(r_odds_pick != super_pick) %>%
    dplyr::select(winner, r_fighter, b_fighter, r_odds, predicted_odds_super, r_odds_pick, super_pick),
  stepwise = final_data %>%
    filter(r_odds_pick != stepwise_pick) %>%
    dplyr::select(r_odds, predicted_odds_stepwise, r_odds_pick, stepwise_pick),
  style = final_data %>%
    filter(r_odds_pick != style_pick) %>%
    dplyr::select(r_odds, predicted_odds_style, r_odds_pick, style_pick)
)

# Add binary predictions for accuracy calculations
final_data <- final_data %>%
  mutate(
    r_odds_prediction = ifelse(r_odds >= 0.5, T, F),
    style_matchup_prediction = ifelse(predicted_odds_style_matchup > 0.5, T, F),
    activity_prediction = ifelse(predicted_odds_activity > 0.5, T, F),
    finishing_prediction = ifelse(predicted_odds_finishing > 0.5, T, F),
    age_prediction = ifelse(predicted_odds_age > 0.5, T, F),
    super_prediction = ifelse(predicted_odds_super > 0.5, T, F),
    stepwise_prediction = ifelse(predicted_odds_stepwise > 0.5, T, F),
    style_prediction = ifelse(predicted_odds_style > 0.5, T, F)
  )

# Calculate accuracy for each model
accuracy_results <- data.frame(
  Model = c("r_odds", "Style Matchup", "Activity", "Finishing", "Age > 35", "Stepwise", "Style","Super"),
  Accuracy = c(
    mean(final_data$r_odds_prediction == final_data$winner),
    mean(final_data$style_matchup_prediction == final_data$winner),
    mean(final_data$activity_prediction == final_data$winner),
    mean(final_data$finishing_prediction == final_data$winner),
    mean(final_data$age_prediction == final_data$winner),
    mean(final_data$stepwise_prediction == final_data$winner),
    mean(final_data$style_prediction == final_data$winner),
    mean(final_data$super_prediction == final_data$winner)
  )
)

# Convert accuracy to percentages and round
accuracy_results$Accuracy <- round(accuracy_results$Accuracy * 100, 2)

mismatch_super <- final_data %>%
  filter(r_odds_pick != super_pick) %>%
  dplyr::select(winner,r_fighter,b_fighter,r_odds, predicted_odds_super, r_odds_pick, super_pick)


difs <- mismatch_super[mismatch_super$predicted_odds_super<0.49|mismatch_super$predicted_odds_super>0.51,]
big_difs <- mismatch_super[mismatch_super$predicted_odds_super<0.4|mismatch_super$predicted_odds_super>0.6,]
diff_choices <-  nrow(difs)/nrow(final_data)
big_diff_choices <- nrow(big_difs)/nrow(final_data)



# Display accuracy results
accuracy_results%>%
  kable(escape = TRUE,digits = 3, caption = "Model Accuracy Summary") %>%
  kable_styling(full_width = FALSE)

results_df_3 <- data.frame(
  Metric = c(
    "% of time super model makes a different choice compared to the odds model",
    "% of time super model makes a different choice, with extreme confidence (|Predicted odds - 0.5| >10)"
  ),
  Value = c(
    paste0(round(diff_choices * 100, 2), "%"),
    paste0(round(big_diff_choices * 100, 2), "%")
  ),
  stringsAsFactors = FALSE
)

results_df_3 %>%
  kable(escape = TRUE,
    caption = "Comparison of Super Model vs Odds Model Choices",
    align = c("l", "r")
  ) %>%
   kable_styling(full_width = FALSE, latex_options = "scale_down")


```

  From now on, the analysis will be focused on the simple base odds model, the stepwise model, and the super model. The above accuracy mechanism is weak in one area, in that it does provide additional benefit to being very confident on high predictions. Relying on a simple threshold (0.5) for predictions may not capture the uncertainty in close fights, or certainty in fights that should not be close. A correct predicted odds of 0.6 has the same function as predicting the odds to be 0.9. The following code attempts to correct this in two potential ways. In the first, log-likelihood values for both the odds model and the supermodel are measured, finding that the super model maintains a higher log-likelihood according to the data. In the second it is observed that the supermodel obtains a lower mean absolute deviation of prediction as compared to the odds model. Both of these results suggest that the supermodel outperforms the betting odds model, but this will be tested in the following section.

```{r, echo=FALSE}
# This code section calculates and compares the log-likelihoods and average absolute deviations
# for the implied odds and model-predicted probabilities.
# -----------------------------------------------------------------------------------

# Ensure the 'winner' column is numeric (convert TRUE/FALSE or factor levels to 1/0)
final_data <- final_data %>%
  mutate(
    winner_numeric = case_when(
      winner == T ~ 1,     # Replace "Red" with the appropriate label
      winner == F ~ 0,    # Replace "Blue" with the appropriate label
      TRUE ~ NA_real_          # Ensure any unexpected values become NA
    )
  )


# Calculate log-likelihoods for implied odds and predicted probabilities
odds_log_likelihood <- log_likelihood(final_data$winner_numeric, final_data$r_odds)
predicted_log_likelihood <- log_likelihood(final_data$winner_numeric, final_data$predicted_odds_super)

# Print log-likelihood comparison
# Calculate mean absolute deviation (confidence measure)
odds_deviation <- mean(abs(final_data$winner_numeric - final_data$r_odds), na.rm = TRUE)
predicted_deviation <- mean(abs(final_data$winner_numeric - final_data$predicted_odds_super), na.rm = TRUE)

# Print deviation comparison

results_df <- data.frame(
  Metric = c(
    "Log-likelihood for implied odds",
    "Log-likelihood for model predictions",
    "Mean absolute deviation for implied odds",
    "Mean absolute deviation for model predictions"
  ),
  Value = c(
    round(odds_log_likelihood, 4),
    round(predicted_log_likelihood, 4),
    round(odds_deviation, 4),
    round(predicted_deviation, 4)
  ),
  stringsAsFactors = FALSE
)
kable(results_df, digits=2,caption = "Comparison Metrics", booktabs = TRUE) %>%
  kable_styling(full_width = FALSE,latex_options = "scale_down")

```

  Next, we perform a likelihood ratio rest to compare the odds model and the super model. This is done to determine if the differences between the two models are statistically significantly different. The likelihood ratio test demonstrates that the difference in log-likelihood for the two models is statistically significant, providing evidence that the super model presents a significant increase in predictive power over the betting odds.

```{r, echo=FALSE}
# This code section performs a likelihood ratio test to compare the fit of two models: a simpler model and a more complex model.
# -----------------------------------------------------------------------------------
 model_0 <- glm(winner ~ r_odds, family = binomial, data = final_data)  # simpler model
model_1 <- lm_super

# Extract the log-likelihoods from each model
logLik_0 <- logLik(model_0)  # Simpler model
logLik_1 <- logLik(model_1)  # Complex model

# Calculate the likelihood ratio statistic
LR_stat <- 2 * (logLik_1 - logLik_0)

# Degrees of freedom: difference in the number of parameters between models
df <- attr(logLik_1, "df") - attr(logLik_0, "df")

# Calculate the p-value using the chi-square distribution
p_value <- pchisq(LR_stat, df = df, lower.tail = FALSE)

# Output the results
# Create a data frame with the metrics
comparison_df <- data.frame(
  Metric = c("Log-Likelihood for Model 0",
             "Log-Likelihood for Model 1",
             "Likelihood Ratio Statistic",
             "Degrees of Freedom",
             "p-value"),
  Value = c(round(logLik_0, 4),
            round(logLik_1, 4),
            round(LR_stat, 4),
            df,
            round(p_value, 4))
)

# Print as a kable
kable(comparison_df, caption = "Model Comparison Metrics") %>%
   kable_styling(full_width = FALSE)

# Interpretation of results
if (p_value < 0.05) {
  cat("The difference in likelihood is statistically significant at the 5% level.\n")
} else {
  cat("The difference in likelihood is not statistically significant at the 5% level.\n")
}


```

  As much of the strength of the super model comes from the inclusion of the stepwise     regression selected variables, then it is important to test whether or not the novel variables significantly produce a benefit in accuracy over the stepwise model. 

```{r, include=FALSE}
# This code section calculates and compares the log-likelihoods, average absolute deviations,
# and performs a likelihood ratio test between the stepwise model and the super model.
# -----------------------------------------------------------------------------------




# Calculate log-likelihoods for predicted probabilities (stepwise and super models)
stepwise_log_likelihood <- log_likelihood(final_data$winner_numeric, final_data$predicted_odds_stepwise)
super_log_likelihood <- log_likelihood(final_data$winner_numeric, final_data$predicted_odds_super)

# Print log-likelihood comparison
cat("Log-likelihood for stepwise model:", round(stepwise_log_likelihood, 4), "\n")
cat("Log-likelihood for super model:", round(super_log_likelihood, 4), "\n")

# Calculate mean absolute deviation (confidence measure)
stepwise_deviation <- mean(abs(final_data$winner_numeric - final_data$predicted_odds_stepwise), na.rm = TRUE)
super_deviation <- mean(abs(final_data$winner_numeric - final_data$predicted_odds_super), na.rm = TRUE)

# Print deviation comparison
cat("Mean absolute deviation for stepwise model:", round(stepwise_deviation, 4), "\n")
cat("Mean absolute deviation for super model:", round(super_deviation, 4), "\n")

# Perform likelihood ratio test
model_stepwise <- lm_stepwise # simpler model
model_super <- lm_super  # Complex model

# Extract the log-likelihoods from each model
logLik_stepwise <- logLik(model_stepwise)  # Simpler model
logLik_super <- logLik(model_super)  # Complex model

# Calculate the likelihood ratio statistic
LR_stat <- 2 * (logLik_super - logLik_stepwise)

# Degrees of freedom: difference in the number of parameters between models
df <- attr(logLik_super, "df") - attr(logLik_stepwise, "df")

# Calculate the p-value using the chi-square distribution
p_value <- pchisq(LR_stat, df = df, lower.tail = FALSE)

# Output the results
comparison_stepwise_df <- data.frame(
  Metric = c(
    "Log-Likelihood for Stepwise Model",
    "Log-Likelihood for Super Model",
    "Likelihood Ratio Statistic",
    "Degrees of Freedom",
    "p-value"
  ),
  Value = c(
    round(logLik_stepwise, 4),
    round(logLik_super, 4),
    round(LR_stat, 4),
    df,
    round(p_value, 4)
  )
)

kable(comparison_stepwise_df, caption = "Stepwise vs. Super Model Comparison") %>%
  kable_styling(full_width = FALSE, latex_options = "scale_down")

# Interpretation of results
if (p_value < 0.05) {
  cat("The difference in likelihood is statistically significant at the 5% level.\n")
} else {
  cat("The difference in likelihood is not statistically significant at the 5% level.\n")
}

# Compare AIC for stepwise and super models
aic_stepwise <- AIC(model_stepwise)
aic_super <- AIC(lm_super)

# Print the AIC values
aic_df <- data.frame(
  Metric = c("AIC for Stepwise Model", "AIC for Super Model"),
  Value = c(round(aic_stepwise, 4), round(aic_super, 4))
)

kable(aic_df, digits=3,caption = "AIC Comparison Between Models") %>%
  kable_styling(full_width = FALSE)

```


  The results indicate that while the super model achieves a slightly better fit to the data compared to the stepwise model, as evidenced by its higher log-likelihood (-1977.503 vs. -1999.2) and lower mean absolute deviation (0.4142 vs. 0.4199), the improvement is not statistically significant. The likelihood ratio statistic (43.3951) with 51 degrees of freedom yields a p-value of 0.7664, which is far above the 0.05 significance threshold. Therefore, there is insufficient evidence to conclude that the super model provides a significantly better fit than the stepwise model, suggesting that the added complexity of the super model may not be justified.

  The results also show that the stepwise model achieves a lower AIC (4048.4) compared to the super model (4107.005), indicating that the stepwise model provides a better balance between model fit and complexity. While the super model has a slightly higher log-likelihood and lower mean absolute deviation, its higher AIC suggests that the additional parameters may not add substantial predictive value. Combined with the non-significant p-value from the likelihood ratio test (0.7664), these findings further support that the increased complexity of the super model may not be warranted.






## Accuracy changes based on 2017 rule change

  In the final part of this section, we look to analyze the impact of the recent major changes to the official judges' scoring criteria, which was implemented on 1/1/2017 (Raimondi). It is possible that the changes to the rules, which aimed to make judging more clear and consistent by emphasizing effective striking and grappling, will help our model predict better by improving the quality and consistency of judges' decisions. Although the gap in accuracy between the super model and the r_odds model appears to widen in the post-2017 period, the lack of statistical significance (p-value \> 0.05) suggests that this observed change could be due to random variation rather than a definitive effect of the rule changes. More data would help determine if recent fights have resulted in further changes to the accuracy gap. Potential future research could involve applying a regression discontinuity approach to this and subsetting for fights that went to a decision rather than including all fights could also help to determine the true effectiveness of the changes.

```{r, echo=FALSE}
# This code section evaluates the impact of the 2017 rule change on model accuracy and computes statistical tests for accuracy gaps.
# -----------------------------------------------------------------------------------

# Split the dataset into pre-2017 and post-2017 subsets
pre2017_data <- final_data %>%
  filter(date < as.Date("2017-01-01"))

post2017_data <- final_data %>%
  filter(date >= as.Date("2017-01-01"))

# Add prediction columns for each model
# Pre-2017 Data
pre2017_data <- pre2017_data %>%
  mutate(
    r_odds_prediction = ifelse(r_odds >= 0.5, 1, 0),
    super_prediction = ifelse(predicted_odds_super > 0.5, 1, 0)
  )

# Post-2017 Data
post2017_data <- post2017_data %>%
  mutate(
    r_odds_prediction = ifelse(r_odds >= 0.5, 1, 0),
    super_prediction = ifelse(predicted_odds_super > 0.5, 1, 0)
  )

# Calculate accuracy for each model
accuracy_r_odds_pre2017 <- mean(pre2017_data$r_odds_prediction == pre2017_data$winner_numeric)
accuracy_super_pre2017 <- mean(pre2017_data$super_prediction == pre2017_data$winner_numeric)

accuracy_r_odds_post2017 <- mean(post2017_data$r_odds_prediction == post2017_data$winner_numeric)
accuracy_super_post2017 <- mean(post2017_data$super_prediction == post2017_data$winner_numeric)

accuracy_change_r_odds <- accuracy_r_odds_post2017 - accuracy_r_odds_pre2017
accuracy_change_super <- accuracy_super_post2017 - accuracy_super_pre2017

# Output accuracy results
accuracy_data <- data.frame(
  Metric = c(
    "Accuracy of r_odds (pre-2017)",
    "Accuracy of super model (pre-2017)",
    "Accuracy of r_odds (post-2017)",
    "Accuracy of super model (post-2017)",
    "Change in r_odds accuracy",
    "Change in super model accuracy"
  ),
  Value = c(
    round(accuracy_r_odds_pre2017 * 100, 2),
    round(accuracy_super_pre2017 * 100, 2),
    round(accuracy_r_odds_post2017 * 100, 2),
    round(accuracy_super_post2017 * 100, 2),
    round(accuracy_change_r_odds * 100, 2),
    round(accuracy_change_super * 100, 2)
  ),
  stringsAsFactors = FALSE
)

# Generate a kable table
kable(accuracy_data, escape=TRUE, booktabs = TRUE,
      caption = "Accuracy Metrics Before and After 2017") %>%
  kable_styling(full_width = FALSE, latex_options = "hold_position") %>%
  column_spec(1, width = "5cm") %>%
  column_spec(2, width = "3cm")

# Sample sizes
n_pre <- nrow(pre2017_data)
n_post <- nrow(post2017_data)

# Compute accuracy gaps
gap_pre <- accuracy_super_pre2017 - accuracy_r_odds_pre2017
gap_post <- accuracy_super_post2017 - accuracy_r_odds_post2017
gap_change <- gap_post - gap_pre

# Compute standard errors
se_pre <- sqrt((accuracy_super_pre2017 * (1 - accuracy_super_pre2017)) / n_pre +
               (accuracy_r_odds_pre2017 * (1 - accuracy_r_odds_pre2017)) / n_pre)
se_post <- sqrt((accuracy_super_post2017 * (1 - accuracy_super_post2017)) / n_post +
                (accuracy_r_odds_post2017 * (1 - accuracy_r_odds_post2017)) / n_post)
se_gap_change <- sqrt(se_pre^2 + se_post^2)

# Compute Z-statistic and p-value
z_gap_change <- gap_change / se_gap_change
p_gap_change <- 2 * (1 - pnorm(abs(z_gap_change)))  # Two-tailed test


# Create a data frame for the gap analysis metrics
gap_analysis <- data.frame(
  Metric = c(
    "Pre-2017 accuracy gap",
    "Post-2017 accuracy gap",
    "Change in accuracy gap",
    "Z-statistic for change in accuracy gap",
    "P-value for change in accuracy gap"
  ),
  Value = c(
    paste0(round(gap_pre * 100, 2), "%"),
    paste0(round(gap_post * 100, 2), "%"),
    paste0(round(gap_change * 100, 2), "%"),
    round(z_gap_change, 2),
    round(p_gap_change, 4)
  ),
  stringsAsFactors = FALSE
)

# Generate the kable table
kable(gap_analysis, escape=TRUE, booktabs = TRUE,
      caption = "Accuracy Gap Analysis Before and After 2017") %>%
  kable_styling(full_width = FALSE, latex_options = "hold_position") %>%
  column_spec(1, width = "6cm") %>%
  column_spec(2, width = "3cm")


```

\newpage

# Conclusion

  The primary goal of this paper was to explore the MMA and UFC betting market to determine if it was efficient, or if it could be improved upon with novel variables and new statistical techniques. This was done using a myriad of statistical and machine learning techniques, namely k-means and hierarchial clustering, factor analysis, logistic regression, principal components analysis, discriminant analysis, random forest models, and boosting algorithms, among others.

  Under the goal of creating novel metrics that detect new information not yet included in betting market lines. To that extent, it can be argued that this analysis was successful. Although very few of the individual metrics were statistically significant, when the novel metrics were appended onto a stepwise regression model, the final model resulted in the highest observed prediction accuracy rate. Cross-validation was employed at every step, ensuring that the results of this are robust and generalizeable to unseen data. The improvement of the super model as compared to the model built off of the odds alone was deemed statistically significant using a likelihood ratio test. Integrating style-based metrics and derived scores (e.g., activity, finishing) added depth not captured by betting odds by building off of expert knowledge to models that better matched with the consensus theory of what contributes to fight outcomes. This implies that publicly available information isn't completely leveraged by bettors. However, our analysis indicated that although the super model achieved lower log-likelihood and mean absolute deviation values, the difference between the stepwise and super models was not statistically significant according to a likelihood ratio test. When combined with the fact that the stepwise model also had a lower AIC, this suggests that the added complexity of the super model in the additional novel variables may have less benefit than previously understood. This may be the result of the "style_matchup" variables a large number of classes, raising what would be a difference of 16 to a difference of 51. 

  The results of these findings can be used by analysts, bettors, or coaches to gain a competitive edge, such as through identifying undervalued fighters or understanding the interplay between fighter style and performance outcomes. For example, both activity score metrics were very statistically significant even when accounting for the odds, suggesting a discontinuity between the broader public's perceived importance of activeness in fighting and the true importance. However, it is possible that these findings could be improved upon in future work. The exclusion of draws and the lack of more recent fights in the final merged data set indicate that more data could be scraped in order to improve our analysis. Additionally, many of the resulting models still face the risk of overfitting, which could be problematic if the historical data the models are fit on are no longer relevant or equivelent to today's fighters.

  Future research could benefit from examining interaction terms, such as those involving weight class, gender, and age, to uncover more precise predictive patterns. Expanding beyond logistic regression, models like Poisson or quasi-Poisson may better capture the dynamics of rate-based variables. Additionally, exploring instrumental variables might provide deeper insights into the determinants of fight outcomes. Research can also attempt to gather data and attempt to build for situation variables like taking short-notice fights or changing weight classes. Lastly, examining the possibility of corruption in judging, particularly its prevalence and patterns in MMA and boxing, offers a compelling avenue for further exploration.

  Overall, while the betting odds provide a strong baseline, our carefully engineered variables can yield statistically and practically meaningful improvements in predictive accuracy. This is illuminating when it comes to understanding the value of a data-driven and analytics approach to understanding MMA fight outcomes, or sports outcomes more broadly, hopefully setting the stage for ongoing innovation in this domain.


\newpage

# Works Cited

Barrasso, Justin. "Alexander Volkanovski Seeking to Break 35-And-over Curse - Sports Illustrated Wrestling News, Analysis and More." Wrestling on Fannation, Sports Illustrated, 10 Feb. 2024, www.si.com/fannation/wrestling/mma/alexander-volkanovski-seeking-to-break-35-and-over-curse. Accessed 11 Dec. 2024.

Bloom, Joe. "All 9 UFC Weight Classes Explained (Men & Women Divisions)." MMA Hive, 23 Feb. 2023, www.mmahive.com/ufc-weight-classes/.

Chugani, Vinod. "The Future of Sports Analytics: Emerging Trends and Technologies." Statology, 18 Oct. 2024, www.statology.org/the-future-of-sports-analytics-emerging-trends-and-technologies/.

Gramm *, Marshall, and Douglas H. Owens. "Determinants of Betting Market Efficiency." Applied Economics Letters, vol. 12, no. 3, Feb. 2005, pp. 181-85, https://doi.org/10.1080/1350485042000314352. Accessed 26 Mar. 2020.

Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.3. https://CRAN.R-project.org/package=stargazer.

Holmes, Benjamin, et al. "A Markov Chain Model for Forecasting Results of Mixed Martial Arts Contests." International Journal of Forecasting, vol. 39, no. 2, Feb. 2022, https://doi.org/10.1016/j.ijforecast.2022.01.007. Accessed 30 May 2022.

Kirk, Christopher. "THE INFLUENCE of AGE and ANTHROPOMETRIC VARIABLES on WINNING and LOSING in PROFESSIONAL MIXED MARTIAL ARTS." Facta Universitatis, Series: Physical Education and Sport, vol. 0, no. 0, 2016, pp. 227-36, casopisi.junis.ni.ac.rs/index.php/FUPhysEdSport/article/view/2070. Accessed 5 Dec. 2024.

mdabbert. "Ultimate UFC Dataset." Kaggle.com, 2024, www.kaggle.com/datasets/mdabbert/ultimate-ufc-dataset.

Raimondi, Marc. "New MMA Rules to Be Implemented Beginning Jan. 1, 2017." MMA Fighting, 3 Aug. 2016, www.mmafighting.com/2016/8/3/12370276/new-mma-rules-to-be-implemented-beginning-jan-1-2017. Accessed 11 Dec. 2024.

"Record Book | UFC." Statleaders.ufc.com, statleaders.ufc.com/en/fight.

Robbins, Thomas R. "Weak Form Efficiency in Sports Betting Markets." American Journal of Management, vol. 23, no. 2, 2023, articlearchives.co/index.php/AJM/article/view/1634. Accessed 3 Dec. 2024.

Stanhope, Stephen. "Are Money Line Odds in UFC Matches Calibrated? Evidence from Events in 2019-2020." SSRN Electronic Journal, 2021, https://doi.org/10.2139/ssrn.3906072. Accessed 5 Mar. 2023.

Warrier , Rajeev. "UFC-Fight Historical Data from 1993 to 2021." Www.kaggle.com, www.kaggle.com/datasets/rajeevw/ufcdata.

Winkelmann, David, et al. "Are Betting Markets Inefficient? Evidence from Simulations and Real Data." Journal of Sports Economics, vol. 25, no. 1, SAGE Publishing, Nov. 2023, pp. 54-97, https://doi.org/10.1177/15270025231204997.

WOODLAND, LINDA M., and BILL M. WOODLAND. "Market Efficiency and the Favorite-Longshot Bias: The Baseball Betting Market." The Journal of Finance, vol. 49, no. 1, Mar. 1994, pp. 269-79, https://doi.org/10.1111/j.1540-6261.1994.tb04429.x.